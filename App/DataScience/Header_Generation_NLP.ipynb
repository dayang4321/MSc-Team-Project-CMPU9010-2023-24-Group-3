{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Text Generation**"
      ],
      "metadata": {
        "id": "bmT2r001Ow-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function generate_header(paragraph) which takes a paragraph of text as input and generates a header from it. It does this by:\n",
        "\n",
        "Tokenizing the paragraph into words.\n",
        "Removing stopwords (common words like \"and\", \"the\", etc.) and punctuation.\n",
        "Finding the five most common significant words.\n",
        "Creating and returning a header composed of these words, capitalized in title case.\n",
        "The function requires NLTK's 'punkt' and 'stopwords' resources, which are downloaded at the beginning."
      ],
      "metadata": {
        "id": "FrzwzxcUOuwP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYn_aHQUwdsC",
        "outputId": "4a93dce8-3859-4392-9793-e98acaa1834d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# Make sure to download these resources from NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def generate_header(paragraph):\n",
        "    # Tokenize the paragraph into words\n",
        "    words = word_tokenize(paragraph)\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
        "\n",
        "    # Find the most common words\n",
        "    most_common_words = Counter(words).most_common(5)\n",
        "\n",
        "    # Create a header using the most common words\n",
        "    header = ' '.join(word for word, count in most_common_words)\n",
        "    return header.title()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code defines a function generate_header_with_ner(paragraph) which generates a header from a paragraph using named entity recognition (NER) with the spaCy NLP library. It processes the paragraph, extracts named entities (like people, places, organizations), identifies the three most common entities, and then creates a header using these entities in title case."
      ],
      "metadata": {
        "id": "eSsIQSs1Ti9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def generate_header_with_ner(paragraph):\n",
        "    # Process the paragraph with spaCy\n",
        "    doc = nlp(paragraph)\n",
        "\n",
        "    # Extract named entities\n",
        "    entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "    # Find the most common entities\n",
        "    most_common_entities = Counter(entities).most_common(3)\n",
        "\n",
        "    # Create a header using the most common entities\n",
        "    header = ' '.join(entity for entity, count in most_common_entities)\n",
        "    return header.title()\n"
      ],
      "metadata": {
        "id": "7Nuq-4w4xZ13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet creates a function generate_header_with_keyphrases(paragraph) that generates a header from a paragraph by identifying key phrases. It uses the RAKE (Rapid Automatic Keyword Extraction) algorithm from the rake_nltk library, which works in conjunction with NLTK's stopwords. The function:\n",
        "\n",
        "Initializes RAKE using NLTK's list of stopwords.\n",
        "Extracts keywords and key phrases from the input paragraph.\n",
        "Selects the top three ranked key phrases.\n",
        "Joins these phrases with a '|' symbol to form a header.\n",
        "Returns the header as the output."
      ],
      "metadata": {
        "id": "gMB0ZNGnWxIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rake-nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEG7yz2gyIwl",
        "outputId": "d3fe9ca9-fa8c-40e5-be03-17bd81f67963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from rake-nltk) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.66.1)\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rake_nltk import Rake\n",
        "import nltk\n",
        "\n",
        "# Make sure to download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def generate_header_with_keyphrases(paragraph):\n",
        "    # Initialize RAKE by using NLTK's stopwords\n",
        "    rake_nltk_var = Rake()\n",
        "\n",
        "    # Extract keywords from text\n",
        "    rake_nltk_var.extract_keywords_from_text(paragraph)\n",
        "    key_phrases = rake_nltk_var.get_ranked_phrases()[:3]  # Get top 3 ranked phrases\n",
        "\n",
        "    # Create a header using the key phrases\n",
        "    header = ' | '.join(key_phrases)\n",
        "    return header\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "BKSPJVxbx6ws",
        "outputId": "da146aff-c0a8-4abe-beca-a5234d9fb7bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2c0f1a31176d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrake_nltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Make sure to download NLTK stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rake_nltk'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function generate_header_from_text(input_text) that generates a header by summarizing the input text. It uses a summarization pipeline from the transformers library, which is based on models like BERT. The function:\n",
        "\n",
        "Loads a summarization pipeline.\n",
        "Checks and manages the length of the input text, truncating it to a maximum of 1024 tokens (a typical limit for BERT-based models).\n",
        "Summarizes the (possibly truncated) text, with specified maximum and minimum lengths for the summary.\n",
        "Returns the first summary as the header."
      ],
      "metadata": {
        "id": "A_U23JYlYbPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def generate_header_from_text(input_text):\n",
        "    # Load a summarization pipeline\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "\n",
        "    # Check and manage text length for the model\n",
        "    max_chunk_size = 1024  # BERT-based models typically have a max length of 1024 tokens\n",
        "    if len(input_text) > max_chunk_size:\n",
        "        # Here you can add a method to split the text into chunks\n",
        "        # For simplicity, we're truncating the text in this example\n",
        "        input_text = input_text[:max_chunk_size]\n",
        "\n",
        "    # Generate summary\n",
        "    summary = summarizer(input_text, max_length=75, min_length=30, do_sample=False)\n",
        "\n",
        "    # Return the first summary (can be used as a header)\n",
        "    return summary[0]['summary_text']"
      ],
      "metadata": {
        "id": "9e5qQPEQzN87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function generate_header_with_t5(input_text) that uses the T5 (Text-To-Text Transfer Transformer) model to generate a header by summarizing the provided text. It works as follows:\n",
        "\n",
        "A T5 model (like 't5-large') and its tokenizer are loaded from the transformers library.\n",
        "The text is preprocessed by prefixing it with \"summarize: \" to instruct the model that summarization is required.\n",
        "The text is encoded to token IDs, with a maximum length of 512 tokens and truncation if necessary.\n",
        "A summary is generated using the model, with specified constraints on the maximum and minimum length of the output.\n",
        "The summarized text is decoded from token IDs to a string, omitting any special tokens.\n",
        "This summarized text is returned as the header."
      ],
      "metadata": {
        "id": "UCf82Uq-b9La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "def generate_header_with_t5(input_text):\n",
        "    model_name = \"t5-large\"  # You can choose other versions like 't5-base' or 't5-large'\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    # Preprocess the text\n",
        "    input_text = \"summarize: \" + input_text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # Generate the output\n",
        "    summary_ids = model.generate(input_ids, max_length=50, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    header = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return header\n"
      ],
      "metadata": {
        "id": "5dE8t-730Ksc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet defines a function generate_header_with_bart(input_text) which uses the BART (Bidirectional and Auto-Regressive Transformers) model for summarization. The BART model and tokenizer from the transformers library are used to generate a concise header from the input text. Here's a quick overview:\n",
        "\n",
        "The BART model and tokenizer specifically the 'facebook/bart-large-cnn' version, are loaded.\n",
        "The input text is encoded to a format suitable for the model, with a maximum length of 1024 tokens, truncating longer texts.\n",
        "The model generates a summary, using a beam search with 4 beams, a maximum output length of 50 tokens, and early stopping for efficiency.\n",
        "The generated summary is decoded back to text, skipping any special tokens.\n",
        "This decoded summary is returned as the header, providing a condensed version of the main content of the input text."
      ],
      "metadata": {
        "id": "Xpjzl-82ddtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "def generate_header_with_bart(input_text):\n",
        "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "    model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "    # Encode the text\n",
        "    inputs = tokenizer([input_text], max_length=1024, return_tensors='pt', truncation=True)\n",
        "\n",
        "    # Generate summary\n",
        "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=50, early_stopping=True)\n",
        "    header = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return header\n",
        "\n"
      ],
      "metadata": {
        "id": "R6HqjtkbCDz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function generate_header_with_gpt2(input_text) uses the GPT-2 model to generate a header for a given text. It follows these steps:\n",
        "\n",
        "Initializes the GPT-2 tokenizer and model.\n",
        "Prepends the input text with a prompt to guide the model to generate a title.\n",
        "Encodes the adjusted input, truncating if necessary.\n",
        "Generates a short text (up to 10 new tokens) using the model.\n",
        "Decodes and returns this generated text as the header. If no output is generated, it returns a default message."
      ],
      "metadata": {
        "id": "Gq5sLHhid-Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "def generate_header_with_gpt2(input_text):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "    # Adjusted prompt for header generation\n",
        "    adjusted_input = f\"Generate a title for the text:\\n\\n{input_text}\"\n",
        "\n",
        "    # Encode input text and truncate if too long\n",
        "    inputs = tokenizer.encode_plus(adjusted_input, return_tensors='pt', truncation=True, max_length=512, add_special_tokens=True)\n",
        "\n",
        "    # Generate text within the model's limits\n",
        "    try:\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_new_tokens=10,  # Control the number of new tokens to generate\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "        # Check if any output was generated\n",
        "        if outputs.shape[0] > 0:\n",
        "            header = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return header\n",
        "        else:\n",
        "            return \"No header generated\"\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return \"Unable to generate header\"\n"
      ],
      "metadata": {
        "id": "its0aazf9_fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generate_header_with_gpt_neo function uses the GPT-Neo model to generate a header for given text. It involves:\n",
        "\n",
        "Initializing the GPT-Neo model and tokenizer.\n",
        "Creating a prompt with the input text for header generation.\n",
        "Encoding and truncating the prompt to a maximum length.\n",
        "Generating a concise output using the model.\n",
        "Decoding and returning the output as a header."
      ],
      "metadata": {
        "id": "-d088ulCfH6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "\n",
        "def generate_header_with_gpt_neo(input_text, model_name=\"EleutherAI/gpt-neo-2.7B\"):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    prompt = f\"Generate a concise and informative header for the following paragraph: {input_text}\"\n",
        "\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "    # Generate the output\n",
        "    output = model.generate(inputs, max_length=60, num_return_sequences=1, temperature=0.7)\n",
        "    header = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    return header.strip()\n"
      ],
      "metadata": {
        "id": "6p7S7-rHCinA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "NZgURgcgTdsQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M37TUJosu3CT"
      },
      "outputs": [],
      "source": [
        "input_text = \"Spoiler alert: this recap is for viewers of Deutschland 83 on Channel 4 and Walter Presents, please refrain from posting details from later episodes if youâ€™ve see more. Catch up on episode four here Now thatâ€™s more like it. This was easily the best episode so far, a tense and dark hour in which a number of characters made decisions that will have far-reaching effects. Most of them bad, if Iâ€™m any judge. I understand those who feel that Deutschland 83 has teetered too far into the absurd to work as a serious drama, but I also felt that this week addressed many of those issues, giving us some good character development into the bargain.Is Martin still the worst spy in the universe? Quite probably, but again Iâ€™d argue that this is part of the point â€“ as much as a spy drama, itâ€™s a story about young people struggling to make the correct choices while being conspicuously and continually let down by authority figures. What the Wingers seem intent on doing is giving us a world populated by people who are trying desperately to do the right thing, but are repeatedly doomed to do wrong by the time in which they are living. The west Poor old Martin. No sooner had he shed his inhibitions in the ashram (and to Bonnie Tyler, no less) than he found himself called back to duty and ordered to Berlin, ostensibly to help out his sick mother by giving her his kidney. Of course, Martinâ€™s missions are never as simple as that, and thus our hero was really in Berlin as a patsy, a convenient way of ensuring that a bunch of explosives was delivered to the terrorist who wanted them â€“ with devastating effect. The bomb itself really happened and was really the work, as stated, of Carlos the Jackal. As to the mysterious planter of the bomb, Iâ€™m presuming he was another patsy of a kind, an associate of Carlosâ€™s who agreed to plant the bomb at the French cultural centre. Also worth noting: the suggestion that the East German government had some involvement in the West German bombing is not a new one and the case against Carlos was partially built from old Stasi files.Martin wasnâ€™t the only one having a bad time this week, as Alex realised that loveâ€™s young dream was not quite as rosy as he had hoped, a situation that led to him storming off from the Mansion of Manipulation, initially seeking solace in the ashram (like everyone else before him) before turning up and offering his services to the DDR. Itâ€™s rather typical of Tobiasâ€™s luck that, even though Alex rejected both his idea of returning to the army and his protestations of love, he ended up offering to spy for the East Germans without Tischbier having to sully his hands by suggesting it. That man is so smooth that even his failures turn into some sort of success. The east Over in the east, betrayal was the name of the game as Annett, not content with rejecting Thomas, chose to sell out his mobile library at the same time. Oh Annett, I will find this very hard to forgive, although I do wonder exactly what message she wanted to get to Martin. Did she think he wasnâ€™t going to turn up and help his mother? Because if that was the reason she contacted Schweppenstette, then I suspect she will end up regretting it. As for poor Thomas, now my favourite character on this show after his impassioned defence of literature, things donâ€™t look good for him at all. That will teach him to go talking about love and literature to the first innocent-looking blonde he meets. Nor are they looking good for Ingrid, even though a rather battered Martin did manage to make it in time to donate the kidney. Will she survive? A couple of weeks ago I would have said sure, but this show is getting notably darker by the week so, despite Lenoraâ€™s intervention, I would now put her chances at 50-50 at best.Stasi files â€¢ Martin really lost his innocence this week â€“ no sooner had he lectured Tobias about being a killer, than he finds himself murdering a stranger on a railway track. I do wonder how much longer he can square his conscience with the life he is having to lead. â€¢ It was also made clear that Tobias was responsible for Lindaâ€™s death â€“ he may not have driven the car, but thereâ€™s no doubting that he gave the order. â€¢ Still, I was impressed by the way in which Martin used my sisterâ€™s old technique of telling the truth so outlandishly that the other person doesnâ€™t believe you. Got us out of many a sticky situation with our parents, I can tell you. â€¢ Lenora (Maria Schrader) almost revealed a heart this week. Despite all the manipulation, I think she genuinely does love her sister, and she really sold the tension as she waited for Martin to arrive. (And Iâ€™m presuming that she is the reason why Martin made it East relatively fast, despite turning up bloody and battered to the checkpoint). â€¢ The Edel home really isnâ€™t a safe place for a fish. After Renate attempted to kill them off with champagne glasses last week, Ursula went a step further this week, dropping one on the ground as a minor act of revenge against her husband. â€¢ Talking of Ursula, I was amused by the relish with which she devoured Alexâ€™s toast. I understand why sheâ€™s protecting her son, but her commitment to that cause was still wildly entertaining. â€¢ One thing I wasnâ€™t sure of â€“ did Ursula intend Frau Netz to think Alex has Aids or was the secretary jumping to conclusions? â€¢ Regarding Frau Netz â€“ do we also think she might be one of Lenoraâ€™s secretaries, or is that a spy reveal too far? â€¢ I loved the way Yvonneâ€™s ashram mate took a moment to centre himself after Alexâ€™s outburst. Sometimes itâ€™s the small things that work. â€¢ I also continue to be amused by Walter Schweppenstette, despite his capacity for evil â€“ I was particularly taken by his discussion about Worcester sauce. Sorry, but Iâ€™m a sucker for a man who knows his condiments. Song of the week Only two songs again this week, but they were both well used. Iâ€™m giving the edge to Mad World by Tears for Fears just because it made this cover from Donnie Darko â€“ an improbable Christmas No 1 â€“ pop into my head for the first time in years. Quote of the week â€œGo on, give each other back rubs while the world goes up in a mushroom cloudâ€ â€“ Alex almost wins my heart with his impassioned take down of hippies. Weâ€™ve all been there, Alex. So what did you think â€“ do you approve of the increasingly dark tone? Who is in worse trouble, Alex or Thomas? What about Martin? Will Ursula continue to lie to her husband? And will Yvonne ever make it out of that ashram? As ever, all speculation and no spoilers welcome below â€¦\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text2 = \"major political event gallop towards u speed bolting horse even two â€“ eu referendum trident debate â€“ running weird direction unpredictable velocity left us u london mayoral election could turn referendum housing issue city could unite many people across fundamental question concentration wealth ever fewer hand way live future look forward way child live see life getting easier harder accept inevitable avenue change ready explore eu referendum could like scottish referendum become conversation good europe â€“ extension good united kingdom â€“ would look like could debate purpose international cooperation could mean wage tax privatisation commonly held asset minded discus post capitalist sharing economy renewable energy moved freely across border fruit technological advance moved beyond appropriation monopoly would place start trident could trigger new thinking foreign policy could allow say loud one blindingly obvious fact cold war condition made threat mass annihilation look like sensible compact hostile nation longer pertain short time could overturn sclerotic norm replace genuinely modern ambitious thinking left could could allow diverted path nowhere chasing stick thrown gleeful opponent sadiq khan big leftie jeremy corbyn ever work together donâ€™t think exactly thing whatâ€™s constitutional basis labour coup left support eu stiffed greece lost humanitarian heart one thing easier leveraging leftiesâ€™ natural anti authoritarianism opening fissure turn chasm britney spear said george bush â€œhonestly think trust president every decision make support â€ fascinated remark remember worrying like scab googling check wording search term â€œbritneyâ€ â€œbushâ€ led wilderness let tell thought interested good conservative yielding authority really interested sense revulsion support anybody every decision made would indivisible dead normal time naturally subversive person never confront question whether capable obedience sort person would obey would never power normal time corbyn avowedly anti establishment leader opposition prevailing convention pretend heâ€™s comical sideshow normal opposition resume due course sooner later reality catch self styled â€œrealisticâ€ view corbynâ€™s pressing problem place find support among people far prefer critique deal â€“ account long take fire someone unable theyâ€™ve agreed heâ€™s right point person le comfortable authority anyone â€“ subject endless scornful commentary yet everyone progressive side â€“ whether parliamentary labour party constituency party member merely sympathetic idea â€“ need consider deal natural reluctance cheerleading allowing u neutralised without question labour mp whose opposition leader implacable disagree profoundly matter principle whether call people tory closet tory misdirected lib dems irrelevant salient point describe full party range also includes mp agree many corbynâ€™s idea worry electability cleave fundamental worry delivery mp worry play corbyn refuse endorse shoot kill rather considering whether shoot kill large enough pressing enough issue divide mp support trident donâ€™t want associated peacenik deeply wedded weapon mass destruction likely â€“ indeed iâ€™d say certainty â€“ conservative mp oppose trident wouldnâ€™t dream making issue ability right gloss internal difference frankly awe inspiring recent debate social housing cameron accused corbyn small c conservative thatâ€™s probably party written cv using insult nobody uttered squeak itâ€™s impressive itâ€™s inimitable left cannot mimic obedience right copy pursuit unity sake need find kind loyalty doesnâ€™t preclude critical distance harmony accommodate audible difference time unique opportunity brought new labour leadership constellation flashpoint could light politics doesnâ€™t need u agree need u\""
      ],
      "metadata": {
        "id": "Ri7Es_x-WWL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text3 = \"another argument often made conscription aside one editorial january â€œit would good young peopleâ€ becomes difficult government engage unpopular war volunteer armed force made largely young working class ethnic minority men limited job opportunity middle upper class parent quite happy see others fighting however child involved perhaps come home body bag much wider objection war certainly case americaâ€™s vietnam venture came end following major demonstration lobbying middle class people political persuasion joseph lockeretz london â€¢ editorial recognising courage refused fight failed mention lion hearted first world war lance corporal william coltmanâ€™s award vc dcm bar mm bar made british armed forcesâ€™ decorated rank distinction still hold deep religious conviction would allow kill served throughout conflict unarmed stretcher bearer personal modesty authoritiesâ€™ problem contradiction hero would take arm mean today bravest soldier people never heard geoff meade st albans hertfordshire â€¢ act refer gave state right â€œforce unmarried men die countryâ€ soon extended married men â€“ may grandfather â€œcalled upâ€ leaving grandmother care five child adrianne leman london â€¢ join debate â€“ email guardian letter theguardian com\""
      ],
      "metadata": {
        "id": "dShv4oxmXE4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text4=\" Java 17, released in September 2021, marks a significant milestone in the evolution of the Java programming language. This version brings forth a plethora of new features, enhancements, and performance improvements, making it a highly anticipated release in the Java community.  In this essay, we will explore the key features of Java 17 and delve into its diverse range of use cases\""
      ],
      "metadata": {
        "id": "ZnJTxi7o-fLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate header\n",
        "header = generate_header(input_text4)\n",
        "print(\"Generated Header:\", header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAUXlEwNwi7h",
        "outputId": "9ac6b4a8-c723-472c-99a6-9ffdbae346fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Header: Java Features Released September Marks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header = generate_header_from_text(input_text4)\n",
        "print(header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk3BlIj1zQZI",
        "outputId": "35d21811-d5e1-4265-8afe-d3f986db2fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Your max_length is set to 75, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Java 17, released in September 2021, marks a significant milestone in the evolution of the Java programming language . This version brings forth a plethora of new features, enhancements, and performance improvements . In this essay, we will explore the key features of Java 17 and delve into its diverse range of use cases .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate header with NER\n",
        "header = generate_header_with_ner(input_text4)\n",
        "print(\"Generated Header with NER:\", header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SSCnHWPxgHu",
        "outputId": "d77e4ce3-0a2d-42bd-c6d7-df175960fc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Header with NER: Java 17 Java September 2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate header with keyphrases\n",
        "header = generate_header_with_keyphrases(input_text4)\n",
        "print(\"Generated Header with Keyphrases:\", header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "n9uvPLkIyUqC",
        "outputId": "4f793aa5-030f-4b21-9385-4ecf4fdc0d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-376dcd8b599e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate header with keyphrases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_header_with_keyphrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generated Header with Keyphrases:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_header_with_keyphrases' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header = generate_header_from_text(input_text4)\n",
        "print(header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d21811-d5e1-4265-8afe-d3f986db2fe4",
        "id": "MLp2qYTfYkVC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Your max_length is set to 75, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Java 17, released in September 2021, marks a significant milestone in the evolution of the Java programming language . This version brings forth a plethora of new features, enhancements, and performance improvements . In this essay, we will explore the key features of Java 17 and delve into its diverse range of use cases .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header = generate_header_with_t5(input_text4)\n",
        "print(header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "sfmVVirl0SD2",
        "outputId": "0e7f9366-bf26-4067-df29-eab05080465b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-dc72b8dabed7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_header_with_t5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-0bb9f6cf2dbe>\u001b[0m in \u001b[0;36mgenerate_header_with_t5\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_header_with_t5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"t5-large\"\u001b[0m  \u001b[0;31m# You can choose other versions like 't5-base' or 't5-large'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"_from_config\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backends\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchecks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "header = generate_header_with_bart(input_text4)\n",
        "print(header)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "876251e6f9594e6c98ae2f49cbdf5080",
            "4c07a23b276145a58e46f1926239e461",
            "c2cbb98663ad44299c71055cf4c82a17",
            "00c46a0947414de5b7a1f1c554245f84",
            "3ef14e9078ab4c7a8d57b753c580982a",
            "94364bd3e99c44a1bde78b45fc202ca4",
            "2342d39d8f11461385760b2ceaa96522",
            "8c05b7b13f1a4cb2bd71875fa54a2b3e",
            "f2dcc9b4fe9848ca97fac423659b945a",
            "ffe0892d39dd4fb5b9f6b53ccbe9679e",
            "40f08ce955174b2ab2deaeb5a059a312",
            "4a6674941dde47b1991b8bb34a1e4191",
            "a7c636db5eae49edaeea4ea03e76e014",
            "53d61102c6494b68aeb02b9abded524d",
            "489482c065454c2e814786b4ce1a9284",
            "20ab0b6002aa4c6cbe829f80bf97f71c",
            "b268002adbab4e5a9019716d994d52bc",
            "684c0793ba0a4eb391e18035834f3a7c",
            "c5cdd7775349484fb9e4a98911f2f0e4",
            "804e1fae53f24b09bae313ddd82a0e05",
            "5a918a74aba24c92a416ce01726130a4",
            "5aa07239aaf24f549b225300b8aa42f6",
            "092985f5577347c980919a41c4d6705d",
            "471bbfc837a34dc6bf7c1fed95558dcf",
            "7a2dd88d3cd0447d98befff82957ca89",
            "5a2e58997e804fe8a08a27cec1bad1ed",
            "5726c8208c2b471aa310dd50345a8254",
            "cfc55f7118194a16b22e645e713c3754",
            "a202cbc30e274ea09d37bc6fc5f77c8b",
            "4f482a521a49458983f4e6c83897f24c",
            "e4078c5683b44f9fb847becfa72ad93a",
            "2a5a7925abec485c87ba948096eeb688",
            "6d716641d44749828335d77b5af3c7fe",
            "3c166e3d35cd43aaa8f8c7831456d2b5",
            "12d505cd28964745abb2f7792bb4c077",
            "116d388fed4a4b83ad4ca1d9f724086f",
            "8d27a12952b24846a1f0353db9bb294e",
            "a879f01eb9bb4fd9b8d84fd7528b930a",
            "0cd25d09b2344cb3a25b45dce1111e9e",
            "17d4e9b892e143508474c724ee7770c6",
            "d950b8723018425eae364817b4c71d12",
            "ef3df54d84bd4fe2ab78c55fce224811",
            "add104b97f6340f28ce55f9cd9535507",
            "f62ce12567ce470aa8e2c224679eb2db",
            "28140b102c7f48009971ac80f63cc8c7",
            "d7c03e866a664704ba5a4c50ef05bbba",
            "cf86e0f7bbbd4568935dc0ac3dd1fc10",
            "3121d5c9bc684a40bd7e8a69bb7bcde4",
            "49390990dc5d4782837d3f119fc03fd0",
            "26578bb5e8ae40bdb24917910689c3a8",
            "ef2282b6f7d34ccd850837e2b7cfa9b2",
            "dc9ccdb9668f4753952428c4c2259cef",
            "1b9c971bf111478abf35e67ee72296cf",
            "d463823976d446f9bf2e5f742e76697e",
            "8045aae5522c4e6abaf5b3b978aa2ade",
            "091453d5ebf24e008df4304484795894",
            "98aaac8af25b4f1992821870ffc6a1df",
            "461f625b7a4a4ba7acc453f624f2030b",
            "f7853c8d50af48a69e4a112f06e3193b",
            "2501131b6b264f5ca7265ba29f4ee1e5",
            "af027e6b92b74f35b0e75cd52266aef1",
            "f2d3ce1633b0401291ab8781e0f160b7",
            "a61cf94424874028b194cc5b47807701",
            "d3b3e564f0c4417b982ac167075dbc90",
            "11e3c24b450c4c5b9f70fd7be2e32698",
            "51a4b306656b43ec87384bdcdb744150"
          ]
        },
        "id": "SOJ-9Pm7CH3t",
        "outputId": "85961edd-e8da-44b2-a51a-592a99c13515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "876251e6f9594e6c98ae2f49cbdf5080"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a6674941dde47b1991b8bb34a1e4191"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "092985f5577347c980919a41c4d6705d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c166e3d35cd43aaa8f8c7831456d2b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28140b102c7f48009971ac80f63cc8c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "091453d5ebf24e008df4304484795894"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1298: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (50). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Java 17, released in September 2021, marks a significant milestone in the evolution of the Java programming language. This version brings forth a plethora of new features, enhancements, and performance improvements. In this essay, we will explore the key features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "upgFphOcd4qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "header = generate_header_with_gpt2(input_text2)\n",
        "print(header)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "8rUG1yJm2cfu",
        "outputId": "12ba6848-316f-4433-f594-d3b86568dc3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-99baa1092f18>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_header_with_gpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'input_text2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "header = generate_header_with_gpt_neo(input_text4)\n",
        "print(header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350,
          "referenced_widgets": [
            "25c990ba7dc14b96a5fcf2dc8301badf",
            "b48adb92c3fe469ca320011a86edbe7b",
            "55416722c1a1468da756d460832634fe",
            "3f19a0b02ce34286a4558c77672d40d4",
            "0dab3e68adf64e71869c848b7c9af377",
            "2232a76a30a74e5287a43567980fd858",
            "d9aeb6e8ac1d479085be40356fea7b06",
            "8984264ca159459b8795cac4da161eed",
            "9171fe3ace5043c595ce0c05830745ea",
            "63509ac433bb412dacb26af2297d7077",
            "46ebfb7b85f544f59ec64baa3ddd3262",
            "32a4dc11c1d747df88bed0cca4f932fb",
            "c8c22712a7e84e5790dcd300d75d6e39",
            "e0edaa9fd2424e228cd6aa383bb381f1",
            "140b5261832f492dbb99b517eea5205f",
            "475e70f77c5146fd949241a29cac56a5",
            "0988895e52044416a1dd5f35814ee590",
            "46c5fa2222484972847b4c779b2246a1",
            "7196dead510c41899122459a8ea2a153",
            "5e0fa860137e4bf4be9486078f8e597c",
            "6a06511238394e428fc43e1a24b5a929",
            "b09b1211df0b4700bbdb3201f36b2554",
            "74dffd8e94a741ce9260e066ab75d425",
            "9e6c7a7abb144cb39c5248c11b082727",
            "a34c2e0e3dda4f8bab44d42595a73ebe",
            "9543b4f5b9ef4282906557311c912920",
            "a5ad73dbf6e845409b55717c85c587ac",
            "10f7d0f1b2964d82a2d2182934251d0f",
            "8b55245a65bc4176b2ea68f10394c3a3",
            "8a352f6666574a2f8154f1d3ac03e548",
            "c0840547d0ec4a888956316c10a30b1d",
            "901c390d1998498a99a35277f70d0ea7",
            "218a48b8456744c7b545767b53c8847c",
            "0b68597a2c9844a188af63f87e1c7426",
            "2c710db2ed3c4419b53c855707c3d691",
            "12a9e2c23fde4c7bac90226269526cee",
            "df8ac0f8fcbd413a98286d8d5fcd78ea",
            "72b67aa11387415caa075bae0165bd39",
            "d89302bec9ca4870881d3669f922a28c",
            "e143b3fe45e54b56b7c64cbf589e638f",
            "a20f33e2fbc94c9ba4171e4755a7b493",
            "b564741d4ab94c73876713d7c22dc9bf",
            "d73c6698c6d94ef6836234ddf58fafdc",
            "deb981c30ec24ba3b7d3d22528d001ba",
            "c0f163b1989d49ea87a2763bebe791db",
            "49feca7de12a412596efe77b9fef1078",
            "2079ca278e114122965f19eb8e0dce61",
            "c83da1fcf5704884aa8daa4fc0f0a15f",
            "808915ccd90240938474d890e9f64a9a",
            "a6a170de37504969964df40b28fc4907",
            "f9be5471dcb14e66af077d077b61dad2",
            "fb395731df7d436cab507fd26d79dd92",
            "00f97f0bfa7f4ea9b2fd999a5b3552ca",
            "b3c8bc8cd1fb40bb8a00dfb00391ff20",
            "c1e6a3b627994b1ea08fbaf3559a0a77",
            "9cd14c8533674816a0eb8cdbe1eb8ad6",
            "ef458ce9dd364d949bf71422ffeaeb38",
            "6263217b8e0d43c5855fb0a45ab2156f",
            "1be4bee6b2ce437ba4349cfa2daf0003",
            "03bcf38acf024b3998093d232b874d0a",
            "5d8fb0a2b86f4203ab0b4dca37993a0a",
            "c33e2962611745a7b0b62ce931cf258f",
            "cc0d2aa0d9ab47ac996407a0997f7896",
            "b51102f655274f5c8573f31f6961c225",
            "9c366cba46684ff4b5d5b9cfcda4619e",
            "4093a97739384f63837a005060316c26"
          ]
        },
        "id": "jQNpXyBKCrNj",
        "outputId": "80e8c749-43e0-4ebe-a9af-79670ed15a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25c990ba7dc14b96a5fcf2dc8301badf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32a4dc11c1d747df88bed0cca4f932fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74dffd8e94a741ce9260e066ab75d425"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b68597a2c9844a188af63f87e1c7426"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0f163b1989d49ea87a2763bebe791db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/10.7G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cd14c8533674816a0eb8cdbe1eb8ad6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 84, but `max_length` is set to 60. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a concise and informative header for the following paragraph:  Java 17, released in September 2021, marks a significant milestone in the evolution of the Java programming language. This version brings forth a plethora of new features, enhancements, and performance improvements, making it a highly anticipated release in the Java community.  In this essay, we will explore the key features of Java 17 and delve into its diverse range of use cases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nftvHqHjfn5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Langchain and OLLAMA"
      ],
      "metadata": {
        "id": "rPsad9Zcfpy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "\n",
        "def generate_header_with_gpt_neo(input_text, model_name=\"EleutherAI/gpt-neo-2.7B\"):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    prompt = f\"Generate a concise and informative header for the following paragraph: {input_text}\"\n",
        "\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "    # Generate the output\n",
        "    output = model.generate(inputs, max_length=60, num_return_sequences=1, temperature=0.7)\n",
        "    header = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    return header.strip()\n"
      ],
      "metadata": {
        "id": "AizFjEpxfu2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "header = generate_header_with_gpt_neo(input_text4)\n",
        "print(header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350,
          "referenced_widgets": [
            "25c990ba7dc14b96a5fcf2dc8301badf",
            "b48adb92c3fe469ca320011a86edbe7b",
            "55416722c1a1468da756d460832634fe",
            "3f19a0b02ce34286a4558c77672d40d4",
            "0dab3e68adf64e71869c848b7c9af377",
            "2232a76a30a74e5287a43567980fd858",
            "d9aeb6e8ac1d479085be40356fea7b06",
            "8984264ca159459b8795cac4da161eed",
            "9171fe3ace5043c595ce0c05830745ea",
            "63509ac433bb412dacb26af2297d7077",
            "46ebfb7b85f544f59ec64baa3ddd3262",
            "32a4dc11c1d747df88bed0cca4f932fb",
            "c8c22712a7e84e5790dcd300d75d6e39",
            "e0edaa9fd2424e228cd6aa383bb381f1",
            "140b5261832f492dbb99b517eea5205f",
            "475e70f77c5146fd949241a29cac56a5",
            "0988895e52044416a1dd5f35814ee590",
            "46c5fa2222484972847b4c779b2246a1",
            "7196dead510c41899122459a8ea2a153",
            "5e0fa860137e4bf4be9486078f8e597c",
            "6a06511238394e428fc43e1a24b5a929",
            "b09b1211df0b4700bbdb3201f36b2554",
            "74dffd8e94a741ce9260e066ab75d425",
            "9e6c7a7abb144cb39c5248c11b082727",
            "a34c2e0e3dda4f8bab44d42595a73ebe",
            "9543b4f5b9ef4282906557311c912920",
            "a5ad73dbf6e845409b55717c85c587ac",
            "10f7d0f1b2964d82a2d2182934251d0f",
            "8b55245a65bc4176b2ea68f10394c3a3",
            "8a352f6666574a2f8154f1d3ac03e548",
            "c0840547d0ec4a888956316c10a30b1d",
            "901c390d1998498a99a35277f70d0ea7",
            "218a48b8456744c7b545767b53c8847c",
            "0b68597a2c9844a188af63f87e1c7426",
            "2c710db2ed3c4419b53c855707c3d691",
            "12a9e2c23fde4c7bac90226269526cee",
            "df8ac0f8fcbd413a98286d8d5fcd78ea",
            "72b67aa11387415caa075bae0165bd39",
            "d89302bec9ca4870881d3669f922a28c",
            "e143b3fe45e54b56b7c64cbf589e638f",
            "a20f33e2fbc94c9ba4171e4755a7b493",
            "b564741d4ab94c73876713d7c22dc9bf",
            "d73c6698c6d94ef6836234ddf58fafdc",
            "deb981c30ec24ba3b7d3d22528d001ba",
            "c0f163b1989d49ea87a2763bebe791db",
            "49feca7de12a412596efe77b9fef1078",
            "2079ca278e114122965f19eb8e0dce61",
            "c83da1fcf5704884aa8daa4fc0f0a15f",
            "808915ccd90240938474d890e9f64a9a",
            "a6a170de37504969964df40b28fc4907",
            "f9be5471dcb14e66af077d077b61dad2",
            "fb395731df7d436cab507fd26d79dd92",
            "00f97f0bfa7f4ea9b2fd999a5b3552ca",
            "b3c8bc8cd1fb40bb8a00dfb00391ff20",
            "c1e6a3b627994b1ea08fbaf3559a0a77",
            "9cd14c8533674816a0eb8cdbe1eb8ad6",
            "ef458ce9dd364d949bf71422ffeaeb38",
            "6263217b8e0d43c5855fb0a45ab2156f",
            "1be4bee6b2ce437ba4349cfa2daf0003",
            "03bcf38acf024b3998093d232b874d0a",
            "5d8fb0a2b86f4203ab0b4dca37993a0a",
            "c33e2962611745a7b0b62ce931cf258f",
            "cc0d2aa0d9ab47ac996407a0997f7896",
            "b51102f655274f5c8573f31f6961c225",
            "9c366cba46684ff4b5d5b9cfcda4619e",
            "4093a97739384f63837a005060316c26"
          ]
        },
        "outputId": "80e8c749-43e0-4ebe-a9af-79670ed15a12",
        "id": "5n9H63Kcfu2m"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25c990ba7dc14b96a5fcf2dc8301badf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32a4dc11c1d747df88bed0cca4f932fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74dffd8e94a741ce9260e066ab75d425"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b68597a2c9844a188af63f87e1c7426"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0f163b1989d49ea87a2763bebe791db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/10.7G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cd14c8533674816a0eb8cdbe1eb8ad6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 84, but `max_length` is set to 60. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a concise and informative header for the following paragraph:  Java 17, released in September 2021, marks a significant milestone in the evolution of the Java programming language. This version brings forth a plethora of new features, enhancements, and performance improvements, making it a highly anticipated release in the Java community.  In this essay, we will explore the key features of Java 17 and delve into its diverse range of use cases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIzo5ImO3PV4",
        "outputId": "d04aa179-5d18-48d1-b5d9-fc477f9ffc08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.343-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-core<0.1,>=0.0.7 (from langchain)\n",
            "  Downloading langchain_core-0.0.7-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.67-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.343 langchain-core-0.0.7 langsmith-0.0.67 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQpsC3t63nLh",
        "outputId": "5b27e21d-62c2-4407-93bf-e2bcdfa345e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  7902    0  7902    0     0  20302      0 --:--:-- --:--:-- --:--:-- 20365\n",
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA GPU. Install lspci or lshw to automatically detect and install NVIDIA CUDA drivers.\n",
            ">>> The Ollama API is now available at 0.0.0.0:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama serve | !ollama run llama2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnhufS5233Zb",
        "outputId": "b29be9c4-13cc-4db9-be35-9773ae01ed31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: !ollama: command not found\n",
            "2023/11/30 15:37:09 images.go:784: total blobs: 0\n",
            "2023/11/30 15:37:09 images.go:791: total unused blobs removed: 0\n",
            "2023/11/30 15:37:09 routes.go:777: Listening on 127.0.0.1:11434 (version 0.1.12)\n",
            "2023/11/30 15:37:09 routes.go:797: warning: gpu support may not be enabled, check that you have installed GPU drivers: nvidia-smi command failed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run llama2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vzY9L2L3uO3",
        "outputId": "5295f33a-e45f-4beb-aad0-1421a4b1ddd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: could not connect to ollama server, run 'ollama serve' to start it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import Ollama\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "llm = Ollama(model=\"llama2\",\n",
        "             callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
        "\n",
        "llm(\"Tell me 5 facts about Roman history:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "KUM17fsI3LbL",
        "outputId": "95e64f63-280c-46a5-ea3f-ffe1fdb44986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             conn.request(\n\u001b[0m\u001b[1;32m    498\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             raise NewConnectionError(\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Failed to establish a new connection: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x78a1f6ca7940>: Failed to establish a new connection: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    846\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78a1f6ca7940>: Failed to establish a new connection: [Errno 111] Connection refused'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a1539d3d2f5b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m              callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tell me 5 facts about Roman history:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    877\u001b[0m             )\n\u001b[1;32m    878\u001b[0m         return (\n\u001b[0;32m--> 879\u001b[0;31m             self.generate(\n\u001b[0m\u001b[1;32m    880\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                 )\n\u001b[1;32m    655\u001b[0m             ]\n\u001b[0;32m--> 656\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             output = (\n\u001b[0;32m--> 530\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    531\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/ollama.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mgenerations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             final_chunk = super()._stream_with_aggregation(\n\u001b[0m\u001b[1;32m    242\u001b[0m                 \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/ollama.py\u001b[0m in \u001b[0;36m_stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     ) -> GenerationChunk:\n\u001b[1;32m    176\u001b[0m         \u001b[0mfinal_chunk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGenerationChunk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstream_resp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstream_resp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stream_response_to_generation_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_resp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/ollama.py\u001b[0m in \u001b[0;36m_create_stream\u001b[0;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m             }\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response = requests.post(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{self.base_url}/api/generate/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78a1f6ca7940>: Failed to establish a new connection: [Errno 111] Connection refused'))"
          ]
        }
      ]
    }
  ]
}