{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Text Generation**"
      ],
      "metadata": {
        "id": "bmT2r001Ow-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function generate_header(paragraph) which takes a paragraph of text as input and generates a header from it. It does this by:\n",
        "\n",
        "Tokenizing the paragraph into words.\n",
        "Removing stopwords (common words like \"and\", \"the\", etc.) and punctuation.\n",
        "Finding the five most common significant words.\n",
        "Creating and returning a header composed of these words, capitalized in title case.\n",
        "The function requires NLTK's 'punkt' and 'stopwords' resources, which are downloaded at the beginning."
      ],
      "metadata": {
        "id": "FrzwzxcUOuwP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYn_aHQUwdsC",
        "outputId": "4a93dce8-3859-4392-9793-e98acaa1834d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# Make sure to download these resources from NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def generate_header(paragraph):\n",
        "    # Tokenize the paragraph into words\n",
        "    words = word_tokenize(paragraph)\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
        "\n",
        "    # Find the most common words\n",
        "    most_common_words = Counter(words).most_common(5)\n",
        "\n",
        "    # Create a header using the most common words\n",
        "    header = ' '.join(word for word, count in most_common_words)\n",
        "    return header.title()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code defines a function generate_header_with_ner(paragraph) which generates a header from a paragraph using named entity recognition (NER) with the spaCy NLP library. It processes the paragraph, extracts named entities (like people, places, organizations), identifies the three most common entities, and then creates a header using these entities in title case."
      ],
      "metadata": {
        "id": "eSsIQSs1Ti9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def generate_header_with_ner(paragraph):\n",
        "    # Process the paragraph with spaCy\n",
        "    doc = nlp(paragraph)\n",
        "\n",
        "    # Extract named entities\n",
        "    entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "    # Find the most common entities\n",
        "    most_common_entities = Counter(entities).most_common(3)\n",
        "\n",
        "    # Create a header using the most common entities\n",
        "    header = ' '.join(entity for entity, count in most_common_entities)\n",
        "    return header.title()\n"
      ],
      "metadata": {
        "id": "7Nuq-4w4xZ13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet creates a function generate_header_with_keyphrases(paragraph) that generates a header from a paragraph by identifying key phrases. It uses the RAKE (Rapid Automatic Keyword Extraction) algorithm from the rake_nltk library, which works in conjunction with NLTK's stopwords. The function:\n",
        "\n",
        "Initializes RAKE using NLTK's list of stopwords.\n",
        "Extracts keywords and key phrases from the input paragraph.\n",
        "Selects the top three ranked key phrases.\n",
        "Joins these phrases with a '|' symbol to form a header.\n",
        "Returns the header as the output."
      ],
      "metadata": {
        "id": "gMB0ZNGnWxIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rake-nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEG7yz2gyIwl",
        "outputId": "d3fe9ca9-fa8c-40e5-be03-17bd81f67963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from rake-nltk) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.66.1)\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rake_nltk import Rake\n",
        "import nltk\n",
        "\n",
        "# Make sure to download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def generate_header_with_keyphrases(paragraph):\n",
        "    # Initialize RAKE by using NLTK's stopwords\n",
        "    rake_nltk_var = Rake()\n",
        "\n",
        "    # Extract keywords from text\n",
        "    rake_nltk_var.extract_keywords_from_text(paragraph)\n",
        "    key_phrases = rake_nltk_var.get_ranked_phrases()[:3]  # Get top 3 ranked phrases\n",
        "\n",
        "    # Create a header using the key phrases\n",
        "    header = ' | '.join(key_phrases)\n",
        "    return header\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "BKSPJVxbx6ws",
        "outputId": "da146aff-c0a8-4abe-beca-a5234d9fb7bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2c0f1a31176d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrake_nltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Make sure to download NLTK stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rake_nltk'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function generate_header_from_text(input_text) that generates a header by summarizing the input text. It uses a summarization pipeline from the transformers library, which is based on models like BERT. The function:\n",
        "\n",
        "Loads a summarization pipeline.\n",
        "Checks and manages the length of the input text, truncating it to a maximum of 1024 tokens (a typical limit for BERT-based models).\n",
        "Summarizes the (possibly truncated) text, with specified maximum and minimum lengths for the summary.\n",
        "Returns the first summary as the header."
      ],
      "metadata": {
        "id": "A_U23JYlYbPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def generate_header_from_text(input_text):\n",
        "    # Load a summarization pipeline\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "\n",
        "    # Check and manage text length for the model\n",
        "    max_chunk_size = 1024  # BERT-based models typically have a max length of 1024 tokens\n",
        "    if len(input_text) > max_chunk_size:\n",
        "        # Here you can add a method to split the text into chunks\n",
        "        # For simplicity, we're truncating the text in this example\n",
        "        input_text = input_text[:max_chunk_size]\n",
        "\n",
        "    # Generate summary\n",
        "    summary = summarizer(input_text, max_length=75, min_length=30, do_sample=False)\n",
        "\n",
        "    # Return the first summary (can be used as a header)\n",
        "    return summary[0]['summary_text']"
      ],
      "metadata": {
        "id": "9e5qQPEQzN87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "NZgURgcgTdsQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M37TUJosu3CT"
      },
      "outputs": [],
      "source": [
        "input_text = \"Spoiler alert: this recap is for viewers of Deutschland 83 on Channel 4 and Walter Presents, please refrain from posting details from later episodes if youâ€™ve see more. Catch up on episode four here Now thatâ€™s more like it. This was easily the best episode so far, a tense and dark hour in which a number of characters made decisions that will have far-reaching effects. Most of them bad, if Iâ€™m any judge. I understand those who feel that Deutschland 83 has teetered too far into the absurd to work as a serious drama, but I also felt that this week addressed many of those issues, giving us some good character development into the bargain.Is Martin still the worst spy in the universe? Quite probably, but again Iâ€™d argue that this is part of the point â€“ as much as a spy drama, itâ€™s a story about young people struggling to make the correct choices while being conspicuously and continually let down by authority figures. What the Wingers seem intent on doing is giving us a world populated by people who are trying desperately to do the right thing, but are repeatedly doomed to do wrong by the time in which they are living. The west Poor old Martin. No sooner had he shed his inhibitions in the ashram (and to Bonnie Tyler, no less) than he found himself called back to duty and ordered to Berlin, ostensibly to help out his sick mother by giving her his kidney. Of course, Martinâ€™s missions are never as simple as that, and thus our hero was really in Berlin as a patsy, a convenient way of ensuring that a bunch of explosives was delivered to the terrorist who wanted them â€“ with devastating effect. The bomb itself really happened and was really the work, as stated, of Carlos the Jackal. As to the mysterious planter of the bomb, Iâ€™m presuming he was another patsy of a kind, an associate of Carlosâ€™s who agreed to plant the bomb at the French cultural centre. Also worth noting: the suggestion that the East German government had some involvement in the West German bombing is not a new one and the case against Carlos was partially built from old Stasi files.Martin wasnâ€™t the only one having a bad time this week, as Alex realised that loveâ€™s young dream was not quite as rosy as he had hoped, a situation that led to him storming off from the Mansion of Manipulation, initially seeking solace in the ashram (like everyone else before him) before turning up and offering his services to the DDR. Itâ€™s rather typical of Tobiasâ€™s luck that, even though Alex rejected both his idea of returning to the army and his protestations of love, he ended up offering to spy for the East Germans without Tischbier having to sully his hands by suggesting it. That man is so smooth that even his failures turn into some sort of success. The east Over in the east, betrayal was the name of the game as Annett, not content with rejecting Thomas, chose to sell out his mobile library at the same time. Oh Annett, I will find this very hard to forgive, although I do wonder exactly what message she wanted to get to Martin. Did she think he wasnâ€™t going to turn up and help his mother? Because if that was the reason she contacted Schweppenstette, then I suspect she will end up regretting it. As for poor Thomas, now my favourite character on this show after his impassioned defence of literature, things donâ€™t look good for him at all. That will teach him to go talking about love and literature to the first innocent-looking blonde he meets. Nor are they looking good for Ingrid, even though a rather battered Martin did manage to make it in time to donate the kidney. Will she survive? A couple of weeks ago I would have said sure, but this show is getting notably darker by the week so, despite Lenoraâ€™s intervention, I would now put her chances at 50-50 at best.Stasi files â€¢ Martin really lost his innocence this week â€“ no sooner had he lectured Tobias about being a killer, than he finds himself murdering a stranger on a railway track. I do wonder how much longer he can square his conscience with the life he is having to lead. â€¢ It was also made clear that Tobias was responsible for Lindaâ€™s death â€“ he may not have driven the car, but thereâ€™s no doubting that he gave the order. â€¢ Still, I was impressed by the way in which Martin used my sisterâ€™s old technique of telling the truth so outlandishly that the other person doesnâ€™t believe you. Got us out of many a sticky situation with our parents, I can tell you. â€¢ Lenora (Maria Schrader) almost revealed a heart this week. Despite all the manipulation, I think she genuinely does love her sister, and she really sold the tension as she waited for Martin to arrive. (And Iâ€™m presuming that she is the reason why Martin made it East relatively fast, despite turning up bloody and battered to the checkpoint). â€¢ The Edel home really isnâ€™t a safe place for a fish. After Renate attempted to kill them off with champagne glasses last week, Ursula went a step further this week, dropping one on the ground as a minor act of revenge against her husband. â€¢ Talking of Ursula, I was amused by the relish with which she devoured Alexâ€™s toast. I understand why sheâ€™s protecting her son, but her commitment to that cause was still wildly entertaining. â€¢ One thing I wasnâ€™t sure of â€“ did Ursula intend Frau Netz to think Alex has Aids or was the secretary jumping to conclusions? â€¢ Regarding Frau Netz â€“ do we also think she might be one of Lenoraâ€™s secretaries, or is that a spy reveal too far? â€¢ I loved the way Yvonneâ€™s ashram mate took a moment to centre himself after Alexâ€™s outburst. Sometimes itâ€™s the small things that work. â€¢ I also continue to be amused by Walter Schweppenstette, despite his capacity for evil â€“ I was particularly taken by his discussion about Worcester sauce. Sorry, but Iâ€™m a sucker for a man who knows his condiments. Song of the week Only two songs again this week, but they were both well used. Iâ€™m giving the edge to Mad World by Tears for Fears just because it made this cover from Donnie Darko â€“ an improbable Christmas No 1 â€“ pop into my head for the first time in years. Quote of the week â€œGo on, give each other back rubs while the world goes up in a mushroom cloudâ€ â€“ Alex almost wins my heart with his impassioned take down of hippies. Weâ€™ve all been there, Alex. So what did you think â€“ do you approve of the increasingly dark tone? Who is in worse trouble, Alex or Thomas? What about Martin? Will Ursula continue to lie to her husband? And will Yvonne ever make it out of that ashram? As ever, all speculation and no spoilers welcome below â€¦\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text2 = \"major political event gallop towards u speed bolting horse even two â€“ eu referendum trident debate â€“ running weird direction unpredictable velocity left us u london mayoral election could turn referendum housing issue city could unite many people across fundamental question concentration wealth ever fewer hand way live future look forward way child live see life getting easier harder accept inevitable avenue change ready explore eu referendum could like scottish referendum become conversation good europe â€“ extension good united kingdom â€“ would look like could debate purpose international cooperation could mean wage tax privatisation commonly held asset minded discus post capitalist sharing economy renewable energy moved freely across border fruit technological advance moved beyond appropriation monopoly would place start trident could trigger new thinking foreign policy could allow say loud one blindingly obvious fact cold war condition made threat mass annihilation look like sensible compact hostile nation longer pertain short time could overturn sclerotic norm replace genuinely modern ambitious thinking left could could allow diverted path nowhere chasing stick thrown gleeful opponent sadiq khan big leftie jeremy corbyn ever work together donâ€™t think exactly thing whatâ€™s constitutional basis labour coup left support eu stiffed greece lost humanitarian heart one thing easier leveraging leftiesâ€™ natural anti authoritarianism opening fissure turn chasm britney spear said george bush â€œhonestly think trust president every decision make support â€ fascinated remark remember worrying like scab googling check wording search term â€œbritneyâ€ â€œbushâ€ led wilderness let tell thought interested good conservative yielding authority really interested sense revulsion support anybody every decision made would indivisible dead normal time naturally subversive person never confront question whether capable obedience sort person would obey would never power normal time corbyn avowedly anti establishment leader opposition prevailing convention pretend heâ€™s comical sideshow normal opposition resume due course sooner later reality catch self styled â€œrealisticâ€ view corbynâ€™s pressing problem place find support among people far prefer critique deal â€“ account long take fire someone unable theyâ€™ve agreed heâ€™s right point person le comfortable authority anyone â€“ subject endless scornful commentary yet everyone progressive side â€“ whether parliamentary labour party constituency party member merely sympathetic idea â€“ need consider deal natural reluctance cheerleading allowing u neutralised without question labour mp whose opposition leader implacable disagree profoundly matter principle whether call people tory closet tory misdirected lib dems irrelevant salient point describe full party range also includes mp agree many corbynâ€™s idea worry electability cleave fundamental worry delivery mp worry play corbyn refuse endorse shoot kill rather considering whether shoot kill large enough pressing enough issue divide mp support trident donâ€™t want associated peacenik deeply wedded weapon mass destruction likely â€“ indeed iâ€™d say certainty â€“ conservative mp oppose trident wouldnâ€™t dream making issue ability right gloss internal difference frankly awe inspiring recent debate social housing cameron accused corbyn small c conservative thatâ€™s probably party written cv using insult nobody uttered squeak itâ€™s impressive itâ€™s inimitable left cannot mimic obedience right copy pursuit unity sake need find kind loyalty doesnâ€™t preclude critical distance harmony accommodate audible difference time unique opportunity brought new labour leadership constellation flashpoint could light politics doesnâ€™t need u agree need u\""
      ],
      "metadata": {
        "id": "Ri7Es_x-WWL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text3 = \"another argument often made conscription aside one editorial january â€œit would good young peopleâ€ becomes difficult government engage unpopular war volunteer armed force made largely young working class ethnic minority men limited job opportunity middle upper class parent quite happy see others fighting however child involved perhaps come home body bag much wider objection war certainly case americaâ€™s vietnam venture came end following major demonstration lobbying middle class people political persuasion joseph lockeretz london â€¢ editorial recognising courage refused fight failed mention lion hearted first world war lance corporal william coltmanâ€™s award vc dcm bar mm bar made british armed forcesâ€™ decorated rank distinction still hold deep religious conviction would allow kill served throughout conflict unarmed stretcher bearer personal modesty authoritiesâ€™ problem contradiction hero would take arm mean today bravest soldier people never heard geoff meade st albans hertfordshire â€¢ act refer gave state right â€œforce unmarried men die countryâ€ soon extended married men â€“ may grandfather â€œcalled upâ€ leaving grandmother care five child adrianne leman london â€¢ join debate â€“ email guardian letter theguardian com\""
      ],
      "metadata": {
        "id": "dShv4oxmXE4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text4=\" Java 17, released in September 2021, marks a significant milestone in the evolution of the Java programming language. This version brings forth a plethora of new features, enhancements, and performance improvements, making it a highly anticipated release in the Java community.  In this essay, we will explore the key features of Java 17 and delve into its diverse range of use cases\""
      ],
      "metadata": {
        "id": "ZnJTxi7o-fLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate header\n",
        "header = generate_header(input_text4)\n",
        "print(\"Generated Header:\", header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAUXlEwNwi7h",
        "outputId": "9ac6b4a8-c723-472c-99a6-9ffdbae346fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Header: Java Features Released September Marks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header = generate_header_from_text(input_text4)\n",
        "print(header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk3BlIj1zQZI",
        "outputId": "35d21811-d5e1-4265-8afe-d3f986db2fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Your max_length is set to 75, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Java 17, released in September 2021, marks a significant milestone in the evolution of the Java programming language . This version brings forth a plethora of new features, enhancements, and performance improvements . In this essay, we will explore the key features of Java 17 and delve into its diverse range of use cases .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate header with NER\n",
        "header = generate_header_with_ner(input_text4)\n",
        "print(\"Generated Header with NER:\", header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SSCnHWPxgHu",
        "outputId": "d77e4ce3-0a2d-42bd-c6d7-df175960fc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Header with NER: Java 17 Java September 2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate header with keyphrases\n",
        "header = generate_header_with_keyphrases(input_text4)\n",
        "print(\"Generated Header with Keyphrases:\", header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "n9uvPLkIyUqC",
        "outputId": "4f793aa5-030f-4b21-9385-4ecf4fdc0d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-376dcd8b599e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate header with keyphrases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_header_with_keyphrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generated Header with Keyphrases:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_header_with_keyphrases' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header = generate_header_from_text(input_text4)\n",
        "print(header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d21811-d5e1-4265-8afe-d3f986db2fe4",
        "id": "MLp2qYTfYkVC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Your max_length is set to 75, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Java 17, released in September 2021, marks a significant milestone in the evolution of the Java programming language . This version brings forth a plethora of new features, enhancements, and performance improvements . In this essay, we will explore the key features of Java 17 and delve into its diverse range of use cases .\n"
          ]
        }
      ]
    }
  ]
}