{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFbV2td2gktG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wk_iFz07GIr",
        "outputId": "385b818c-6f9e-4afc-ac9a-a13c6430bc5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from scipy import sparse\n",
        "import string\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(2)\n",
        "from numpy.random import seed\n",
        "seed(1)"
      ],
      "metadata": {
        "id": "vKIdY35l7JJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"https://drive.google.com/drive/folders/1_RGwmjvW8p3jHuKqIbbHXdBDuDl2DTjU?usp=drive_link\""
      ],
      "metadata": {
        "id": "HinlvqOH7jv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder (data_url, quiet=True, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBGscuMy7g3F",
        "outputId": "c1bec2a4-c37c-4c44-8965-a9f90a797ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Guardian News Articles/guardian_articles.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data_sample = pd.read_csv('/content/Guardian News Articles/guardian_articles.csv')\n",
        "\n",
        "# Using only 10% of the data\n",
        "data = data_sample.sample(frac=0.01, random_state=42)\n",
        "\n",
        "# Drop rows with missing values\n",
        "data.dropna(subset=['webTitle', 'bodyContent'], inplace=True)"
      ],
      "metadata": {
        "id": "tVIY5vNwBvBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function for advanced text cleaning\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstruct the text from tokens\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply advanced text preprocessing\n",
        "data['webTitle'] = data['webTitle'].apply(clean_text)\n",
        "data['bodyContent'] = data['bodyContent'].apply(clean_text)\n",
        "\n",
        "# Add <start> and <end> tokens to each headline\n",
        "data['webTitle'] = data['webTitle'].apply(lambda x: '<start> ' + x + ' <end>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xm3KbhbG1R7",
        "outputId": "430d22cd-dfa7-4556-e8ec-fed122f8a5cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['webTitle']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8QZB61IBUj9",
        "outputId": "577ae6b3-f0bc-4362-dabb-9752f4009aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118646    <start> demand tory mp scrap covid rule could ...\n",
              "124284    <start> really smart motorway would lower spee...\n",
              "141061    <start> ‘profiting suffering’ ap cancel sale m...\n",
              "80327     <start> school prison pipeline criminal justic...\n",
              "96867     <start> naked coronavirus tale desperation mad...\n",
              "                                ...                        \n",
              "140283    <start> uk new zealand sign free trade deal <end>\n",
              "110667    <start> crisis stage texas border city reel co...\n",
              "1812      <start> apple iphone sale projected stagnant q...\n",
              "91415      <start> woke gammon buzzword people coined <end>\n",
              "31590     <start> arsène wenger say arsenal fa cup win a...\n",
              "Name: webTitle, Length: 1486, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer with a smaller vocabulary\n",
        "max_vocab_size = 50  # Adjust as needed\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
        "tokenizer.fit_on_texts(data['webTitle'])\n",
        "tokenizer.fit_on_texts(data['bodyContent'])\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences_body = tokenizer.texts_to_sequences(data['bodyContent'])\n",
        "sequences_title = tokenizer.texts_to_sequences(data['webTitle'])\n",
        "\n",
        "# Padding sequences\n",
        "body_padded = pad_sequences(sequences_body, maxlen=300)  # Adjust maxlen as per your data\n",
        "title_padded = pad_sequences(sequences_title, maxlen=30, padding='post')  # Adjust maxlen as per your data\n",
        "\n",
        "# Preparing decoder input\n",
        "title_padded_shifted = np.zeros_like(title_padded)\n",
        "title_padded_shifted[:, 1:] = title_padded[:, :-1]"
      ],
      "metadata": {
        "id": "i7WKnqyKFzib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into training and validation sets\n",
        "X_train, X_val, title_train, title_val, decoder_input_train, decoder_input_val = train_test_split(\n",
        "    body_padded, title_padded, title_padded_shifted, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7YyvwRBig9YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "385b818c-6f9e-4afc-ac9a-a13c6430bc5a",
        "id": "EZUOmWP4i10P"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from scipy import sparse\n",
        "import string\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(2)\n",
        "from numpy.random import seed\n",
        "seed(1)"
      ],
      "metadata": {
        "id": "1Ow-ztU_i10Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"https://drive.google.com/drive/folders/1_RGwmjvW8p3jHuKqIbbHXdBDuDl2DTjU?usp=drive_link\""
      ],
      "metadata": {
        "id": "jc0_olxki10Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder (data_url, quiet=True, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1bec2a4-c37c-4c44-8965-a9f90a797ece",
        "id": "BNe2bDAgi10Q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Guardian News Articles/guardian_articles.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data_sample = pd.read_csv('/content/Guardian News Articles/guardian_articles.csv')\n",
        "\n",
        "# Using only 10% of the data\n",
        "data = data_sample.sample(frac=0.01, random_state=42)\n",
        "\n",
        "# Drop rows with missing values\n",
        "data.dropna(subset=['webTitle', 'bodyContent'], inplace=True)"
      ],
      "metadata": {
        "id": "KRCaOza4i10Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function for advanced text cleaning\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstruct the text from tokens\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply advanced text preprocessing\n",
        "data['webTitle'] = data['webTitle'].apply(clean_text)\n",
        "data['bodyContent'] = data['bodyContent'].apply(clean_text)\n",
        "\n",
        "# Add <start> and <end> tokens to each headline\n",
        "data['webTitle'] = data['webTitle'].apply(lambda x: '<start> ' + x + ' <end>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430d22cd-dfa7-4556-e8ec-fed122f8a5cd",
        "id": "2hG_70vsi10R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['webTitle']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577ae6b3-f0bc-4362-dabb-9752f4009aef",
        "id": "OfxZOk8Fi10R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118646    <start> demand tory mp scrap covid rule could ...\n",
              "124284    <start> really smart motorway would lower spee...\n",
              "141061    <start> ‘profiting suffering’ ap cancel sale m...\n",
              "80327     <start> school prison pipeline criminal justic...\n",
              "96867     <start> naked coronavirus tale desperation mad...\n",
              "                                ...                        \n",
              "140283    <start> uk new zealand sign free trade deal <end>\n",
              "110667    <start> crisis stage texas border city reel co...\n",
              "1812      <start> apple iphone sale projected stagnant q...\n",
              "91415      <start> woke gammon buzzword people coined <end>\n",
              "31590     <start> arsène wenger say arsenal fa cup win a...\n",
              "Name: webTitle, Length: 1486, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer with a smaller vocabulary\n",
        "max_vocab_size = 50  # Adjust as needed\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
        "tokenizer.fit_on_texts(data['webTitle'])\n",
        "tokenizer.fit_on_texts(data['bodyContent'])\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences_body = tokenizer.texts_to_sequences(data['bodyContent'])\n",
        "sequences_title = tokenizer.texts_to_sequences(data['webTitle'])\n",
        "\n",
        "# Padding sequences\n",
        "body_padded = pad_sequences(sequences_body, maxlen=300)  # Adjust maxlen as per your data\n",
        "title_padded = pad_sequences(sequences_title, maxlen=30, padding='post')  # Adjust maxlen as per your data\n",
        "\n",
        "# Preparing decoder input\n",
        "title_padded_shifted = np.zeros_like(title_padded)\n",
        "title_padded_shifted[:, 1:] = title_padded[:, :-1]"
      ],
      "metadata": {
        "id": "8Uyzmusni10R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into training and validation sets\n",
        "X_train, X_val, title_train, title_val, decoder_input_train, decoder_input_val = train_test_split(\n",
        "    body_padded, title_padded, title_padded_shifted, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "1Y5ZDc6Ui10R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert to sparse categorical\n",
        "def to_sparse_categorical(y, num_classes):\n",
        "    y_sparse = []\n",
        "    for sequence in y:\n",
        "        y_sparse_seq = np.zeros((len(sequence), num_classes))\n",
        "        for i, idx in enumerate(sequence):\n",
        "            if idx < num_classes:\n",
        "                y_sparse_seq[i, idx] = 1\n",
        "        y_sparse.append(y_sparse_seq)\n",
        "    return np.array(y_sparse)\n",
        "\n",
        "# Converting training and validation target data to sparse categorical\n",
        "num_classes = max_vocab_size + 1  # +1 for the padding token\n",
        "y_train_sparse = to_sparse_categorical(title_train, num_classes)\n",
        "y_val_sparse = to_sparse_categorical(title_val, num_classes)"
      ],
      "metadata": {
        "id": "H3eR1eb5MKoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Bidirectional\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(input_dim=max_vocab_size + 1, output_dim=128)\n",
        "encoder_lstm = Bidirectional(LSTM(128, return_state=True))\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding_layer = Embedding(input_dim=max_vocab_size + 1, output_dim=128)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_dense = Dense(max_vocab_size + 1, activation='softmax')  # Correct number of units\n",
        "\n",
        "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Seq2Seq Model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n"
      ],
      "metadata": {
        "id": "pa65WJMqBu9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "PV8zb5aEDYoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([X_train, decoder_input_train], y_train_sparse,\n",
        "                    validation_data=([X_val, decoder_input_val], y_val_sparse),\n",
        "                    epochs=5,  # Adjust as needed\n",
        "                    batch_size=32)"
      ],
      "metadata": {
        "id": "85csh9VyBu6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, load_model\n",
        "import pickle\n",
        "#saving model\n",
        "model.save('/content/drive/MyDrive/path/Models/seq2seq_headline_generator/seq2seq_headline_generator.h5')\n",
        "with open('/content/drive/My Drive/path/Models/seq2seq_headline_generator/history.p', 'wb') as f:\n",
        "    pickle.dump(history.history, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9EsR7nAizNj",
        "outputId": "c5f63c55-828b-4124-d308-801b5b62d028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}