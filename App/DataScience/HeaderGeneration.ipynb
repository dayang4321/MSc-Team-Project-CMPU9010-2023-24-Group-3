{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "385b818c-6f9e-4afc-ac9a-a13c6430bc5a",
        "id": "EZUOmWP4i10P"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from scipy import sparse\n",
        "import string\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(2)\n",
        "from numpy.random import seed\n",
        "seed(1)"
      ],
      "metadata": {
        "id": "1Ow-ztU_i10Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtQbvBxDIq4y"
      },
      "source": [
        "This code block is used to mount your Google Drive to the Colab environment. It allows you to access files stored in your Google Drive directly from the Colab notebook. The force_remount=True parameter ensures that the drive is remounted if it was previously mounted during the session. This is useful for ensuring that the latest version of the drive's contents is accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c72c502-3137-402a-e493-6fafd59f4cc5",
        "id": "VVg6UEsLJ7Mj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TsVvmSjIzYL"
      },
      "source": [
        "This code block imports essential libraries and modules for data processing, machine learning model building, and text analysis. It includes libraries for data manipulation (Pandas), neural network construction (Keras, TensorFlow), natural language processing (NLTK), and various utilities for handling arrays, strings, and file paths. Additionally, it sets the seed for random number generation to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OFsBI1uJ7Mj"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from scipy import sparse\n",
        "import string\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(2)\n",
        "from numpy.random import seed\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgSV__GOoDoR"
      },
      "outputs": [],
      "source": [
        "# data_url = \"something\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iMpIeV-K4AZ"
      },
      "source": [
        "This line of code assigns a URL to the variable data_url. The URL points to a Google Drive folder containing the data files needed for the project. The data can be accessed and downloaded using this URL, typically in combination with Google Drive APIs or tools like gdown to facilitate the download process in a programming environment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"https://drive.google.com/drive/folders/1_RGwmjvW8p3jHuKqIbbHXdBDuDl2DTjU?usp=drive_link\""
      ],
      "metadata": {
        "id": "jc0_olxki10Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder (data_url, quiet=True, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1bec2a4-c37c-4c44-8965-a9f90a797ece",
        "id": "BNe2bDAgi10Q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Guardian News Articles/guardian_articles.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data_sample = pd.read_csv('/content/Guardian News Articles/guardian_articles.csv')\n",
        "\n",
        "# Using only 10% of the data\n",
        "data = data_sample.sample(frac=0.01, random_state=42)\n",
        "\n",
        "# Drop rows with missing values\n",
        "data.dropna(subset=['webTitle', 'bodyContent'], inplace=True)"
      ],
      "metadata": {
        "id": "KRCaOza4i10Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3Ef5JXnLcUf"
      },
      "outputs": [],
      "source": [
        "data_url = \"https://drive.google.com/drive/folders/1_RGwmjvW8p3jHuKqIbbHXdBDuDl2DTjU?usp=drive_link\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfH94Bm6LFI_"
      },
      "source": [
        "This code uses the gdown library to download an entire folder from Google Drive. The data_url specifies the location of the folder on Google Drive. The quiet=True parameter ensures that the download process runs without printing unnecessary logs, and use_cookies=False indicates that cookies should not be used in the download process. This is a convenient way to programmatically download all files from a shared Google Drive folder directly into the runtime environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124707be-bc99-426b-f92b-9c65876452c1",
        "id": "zi5rayOlLcUg"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Guardian News Articles/cleaned_guardian_articles_test.csv',\n",
              " '/content/Guardian News Articles/cleaned_guardian_articles_train.csv',\n",
              " '/content/Guardian News Articles/cleaned_guardian_articles.csv',\n",
              " '/content/Guardian News Articles/guardian_articles.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "gdown.download_folder (data_url, quiet=True, use_cookies=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNwdxLv0LVyC"
      },
      "source": [
        "This code block is responsible for loading and cleaning a dataset. It first reads a CSV file named 'guardian_articles.csv' from the specified directory into a pandas DataFrame called data_sample. After loading the data, it removes rows where values in either the 'webTitle' or 'bodyContent' columns are missing. This cleaning step is crucial to ensure the quality and reliability of the dataset for further processing and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJWQDtPHLcUg"
      },
      "outputs": [],
      "source": [
        "# # Load dataset\n",
        "# data = pd.read_csv('/content/Guardian News Articles/guardian_articles.csv')\n",
        "\n",
        "# # Using only 10% of the data\n",
        "# # data = data_sample.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# # Drop rows with missing values\n",
        "# data.dropna(subset=['webTitle', 'bodyContent'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G4rUCDFLZoG"
      },
      "source": [
        "This code block sets up the necessary components for advanced text preprocessing. It starts by downloading NLTK datasets for stopwords and WordNet, which are used for text cleaning and normalization. A function clean_text is defined to convert text to lowercase, remove punctuation and numbers, eliminate extra spaces, tokenize, remove stopwords, and apply lemmatization. This function is applied to the 'webTitle' and 'bodyContent' columns of the dataset to clean and preprocess the text. Additionally, special tokens 'starttoken' and 'endtoken' are added to each headline in 'webTitle' to signify the beginning and end, which is a common technique in natural language processing, especially in sequence modeling tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buk_6FCP-RK6"
      },
      "outputs": [],
      "source": [
        "# Load the cleaned dataset from a CSV file\n",
        "data = pd.read_csv('/content/Guardian News Articles/cleaned_guardian_articles_train.csv')\n",
        "\n",
        "# Drop rows with missing values\n",
        "data.dropna(subset=['webTitle', 'bodyContent','webTitleToken'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "rD8BLqU3NXlQ",
        "outputId": "9ea9709b-2819-40b4-8c13-167dd410c166"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               article_id         sectionName  \\\n",
              "0       world/2016/jan/31/church-christian-british-adu...          World news   \n",
              "1       tv-and-radio/2016/jan/31/war-and-peace-recap-e...  Television & radio   \n",
              "2       sport/2016/jan/31/angelique-kerber-serena-will...               Sport   \n",
              "3       football/2016/jan/31/fulham-call-off-moussa-de...            Football   \n",
              "4          sport/2016/jan/31/kieran-brookes-england-squad               Sport   \n",
              "...                                                   ...                 ...   \n",
              "148719  world/2022/jun/21/marble-head-of-hercules-pull...          World news   \n",
              "148720  music/2022/jun/22/i-got-sick-of-talking-about-...               Music   \n",
              "148721  australia-news/2022/jun/22/the-small-town-with...      Australia news   \n",
              "148722  australia-news/2022/jun/22/power-to-ban-citize...      Australia news   \n",
              "148723  australia-news/2022/jun/22/liberal-mps-say-pet...      Australia news   \n",
              "\n",
              "                                                 webTitle  \\\n",
              "0             half british adult visited church past year   \n",
              "1       war peace recap episode five – hero leech cast...   \n",
              "2       angelique kerber aim dislodge serena williams ...   \n",
              "3       fulham call moussa dembéle’s £m move tottenham...   \n",
              "4       northampton kieran brooke drafted england squa...   \n",
              "...                                                   ...   \n",
              "148719  marble head hercules pulled roman shipwreck si...   \n",
              "148720  ‘i got sick talking myself’ spacey jane back m...   \n",
              "148721   small town big potato inspired global poetry win   \n",
              "148722  power ban citizen entering australia questione...   \n",
              "148723  liberal mp say peter dutton let party room dec...   \n",
              "\n",
              "                                                   webUrl  \\\n",
              "0       https://www.theguardian.com/world/2016/jan/31/...   \n",
              "1       https://www.theguardian.com/tv-and-radio/2016/...   \n",
              "2       https://www.theguardian.com/sport/2016/jan/31/...   \n",
              "3       https://www.theguardian.com/football/2016/jan/...   \n",
              "4       https://www.theguardian.com/sport/2016/jan/31/...   \n",
              "...                                                   ...   \n",
              "148719  https://www.theguardian.com/world/2022/jun/21/...   \n",
              "148720  https://www.theguardian.com/music/2022/jun/22/...   \n",
              "148721  https://www.theguardian.com/australia-news/202...   \n",
              "148722  https://www.theguardian.com/australia-news/202...   \n",
              "148723  https://www.theguardian.com/australia-news/202...   \n",
              "\n",
              "                                              bodyContent  \\\n",
              "0       half british adult visited church past year de...   \n",
              "1       ‘strange thing turn sometimes …’ pierre get aw...   \n",
              "2       gone midnight angelique kerber conducting yet ...   \n",
              "3       fulham angrily called moussa dembélé’s propose...   \n",
              "4       england drafted northampton prop kieran brooke...   \n",
              "...                                                   ...   \n",
              "148719  archaeologist it’s underwater find keep giving...   \n",
              "148720  mop curl caleb harper – spacey jane frontman g...   \n",
              "148721  robertson small pretty town perched edge new s...   \n",
              "148722  high court decision striking home affair minis...   \n",
              "148723  liberal mp urging peter dutton let party room ...   \n",
              "\n",
              "          webPublicationDate        id  \\\n",
              "0       2016-01-31T22:00:09Z      21.0   \n",
              "1       2016-01-31T22:00:09Z      22.0   \n",
              "2       2016-01-31T21:59:09Z      23.0   \n",
              "3       2016-01-31T21:56:54Z      24.0   \n",
              "4       2016-01-31T21:52:14Z      25.0   \n",
              "...                      ...       ...   \n",
              "148719  2022-06-21T17:31:32Z  149835.0   \n",
              "148720  2022-06-21T17:30:09Z  149836.0   \n",
              "148721  2022-06-21T17:30:09Z  149837.0   \n",
              "148722  2022-06-21T17:30:08Z  149838.0   \n",
              "148723  2022-06-21T17:30:08Z  149839.0   \n",
              "\n",
              "                                            webTitleToken  \n",
              "0       starttoken half british adult visited church p...  \n",
              "1       starttoken war peace recap episode five – hero...  \n",
              "2       starttoken angelique kerber aim dislodge seren...  \n",
              "3       starttoken fulham call moussa dembéle’s £m mov...  \n",
              "4       starttoken northampton kieran brooke drafted e...  \n",
              "...                                                   ...  \n",
              "148719  starttoken marble head hercules pulled roman s...  \n",
              "148720  starttoken ‘i got sick talking myself’ spacey ...  \n",
              "148721  starttoken small town big potato inspired glob...  \n",
              "148722  starttoken power ban citizen entering australi...  \n",
              "148723  starttoken liberal mp say peter dutton let par...  \n",
              "\n",
              "[148695 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eea36b58-8e77-40d7-b45e-7c4ec6ea2a3b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>sectionName</th>\n",
              "      <th>webTitle</th>\n",
              "      <th>webUrl</th>\n",
              "      <th>bodyContent</th>\n",
              "      <th>webPublicationDate</th>\n",
              "      <th>id</th>\n",
              "      <th>webTitleToken</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>world/2016/jan/31/church-christian-british-adu...</td>\n",
              "      <td>World news</td>\n",
              "      <td>half british adult visited church past year</td>\n",
              "      <td>https://www.theguardian.com/world/2016/jan/31/...</td>\n",
              "      <td>half british adult visited church past year de...</td>\n",
              "      <td>2016-01-31T22:00:09Z</td>\n",
              "      <td>21.0</td>\n",
              "      <td>starttoken half british adult visited church p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tv-and-radio/2016/jan/31/war-and-peace-recap-e...</td>\n",
              "      <td>Television &amp; radio</td>\n",
              "      <td>war peace recap episode five – hero leech cast...</td>\n",
              "      <td>https://www.theguardian.com/tv-and-radio/2016/...</td>\n",
              "      <td>‘strange thing turn sometimes …’ pierre get aw...</td>\n",
              "      <td>2016-01-31T22:00:09Z</td>\n",
              "      <td>22.0</td>\n",
              "      <td>starttoken war peace recap episode five – hero...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport/2016/jan/31/angelique-kerber-serena-will...</td>\n",
              "      <td>Sport</td>\n",
              "      <td>angelique kerber aim dislodge serena williams ...</td>\n",
              "      <td>https://www.theguardian.com/sport/2016/jan/31/...</td>\n",
              "      <td>gone midnight angelique kerber conducting yet ...</td>\n",
              "      <td>2016-01-31T21:59:09Z</td>\n",
              "      <td>23.0</td>\n",
              "      <td>starttoken angelique kerber aim dislodge seren...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>football/2016/jan/31/fulham-call-off-moussa-de...</td>\n",
              "      <td>Football</td>\n",
              "      <td>fulham call moussa dembéle’s £m move tottenham...</td>\n",
              "      <td>https://www.theguardian.com/football/2016/jan/...</td>\n",
              "      <td>fulham angrily called moussa dembélé’s propose...</td>\n",
              "      <td>2016-01-31T21:56:54Z</td>\n",
              "      <td>24.0</td>\n",
              "      <td>starttoken fulham call moussa dembéle’s £m mov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sport/2016/jan/31/kieran-brookes-england-squad</td>\n",
              "      <td>Sport</td>\n",
              "      <td>northampton kieran brooke drafted england squa...</td>\n",
              "      <td>https://www.theguardian.com/sport/2016/jan/31/...</td>\n",
              "      <td>england drafted northampton prop kieran brooke...</td>\n",
              "      <td>2016-01-31T21:52:14Z</td>\n",
              "      <td>25.0</td>\n",
              "      <td>starttoken northampton kieran brooke drafted e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148719</th>\n",
              "      <td>world/2022/jun/21/marble-head-of-hercules-pull...</td>\n",
              "      <td>World news</td>\n",
              "      <td>marble head hercules pulled roman shipwreck si...</td>\n",
              "      <td>https://www.theguardian.com/world/2022/jun/21/...</td>\n",
              "      <td>archaeologist it’s underwater find keep giving...</td>\n",
              "      <td>2022-06-21T17:31:32Z</td>\n",
              "      <td>149835.0</td>\n",
              "      <td>starttoken marble head hercules pulled roman s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148720</th>\n",
              "      <td>music/2022/jun/22/i-got-sick-of-talking-about-...</td>\n",
              "      <td>Music</td>\n",
              "      <td>‘i got sick talking myself’ spacey jane back m...</td>\n",
              "      <td>https://www.theguardian.com/music/2022/jun/22/...</td>\n",
              "      <td>mop curl caleb harper – spacey jane frontman g...</td>\n",
              "      <td>2022-06-21T17:30:09Z</td>\n",
              "      <td>149836.0</td>\n",
              "      <td>starttoken ‘i got sick talking myself’ spacey ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148721</th>\n",
              "      <td>australia-news/2022/jun/22/the-small-town-with...</td>\n",
              "      <td>Australia news</td>\n",
              "      <td>small town big potato inspired global poetry win</td>\n",
              "      <td>https://www.theguardian.com/australia-news/202...</td>\n",
              "      <td>robertson small pretty town perched edge new s...</td>\n",
              "      <td>2022-06-21T17:30:09Z</td>\n",
              "      <td>149837.0</td>\n",
              "      <td>starttoken small town big potato inspired glob...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148722</th>\n",
              "      <td>australia-news/2022/jun/22/power-to-ban-citize...</td>\n",
              "      <td>Australia news</td>\n",
              "      <td>power ban citizen entering australia questione...</td>\n",
              "      <td>https://www.theguardian.com/australia-news/202...</td>\n",
              "      <td>high court decision striking home affair minis...</td>\n",
              "      <td>2022-06-21T17:30:08Z</td>\n",
              "      <td>149838.0</td>\n",
              "      <td>starttoken power ban citizen entering australi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148723</th>\n",
              "      <td>australia-news/2022/jun/22/liberal-mps-say-pet...</td>\n",
              "      <td>Australia news</td>\n",
              "      <td>liberal mp say peter dutton let party room dec...</td>\n",
              "      <td>https://www.theguardian.com/australia-news/202...</td>\n",
              "      <td>liberal mp urging peter dutton let party room ...</td>\n",
              "      <td>2022-06-21T17:30:08Z</td>\n",
              "      <td>149839.0</td>\n",
              "      <td>starttoken liberal mp say peter dutton let par...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>148695 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eea36b58-8e77-40d7-b45e-7c4ec6ea2a3b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eea36b58-8e77-40d7-b45e-7c4ec6ea2a3b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eea36b58-8e77-40d7-b45e-7c4ec6ea2a3b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-93ff180e-fa20-4871-92aa-547a0c7d1723\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-93ff180e-fa20-4871-92aa-547a0c7d1723')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-93ff180e-fa20-4871-92aa-547a0c7d1723 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function for advanced text cleaning\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstruct the text from tokens\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply advanced text preprocessing\n",
        "data['webTitle'] = data['webTitle'].apply(clean_text)\n",
        "data['bodyContent'] = data['bodyContent'].apply(clean_text)\n",
        "\n",
        "# Add <start> and <end> tokens to each headline\n",
        "data['webTitle'] = data['webTitle'].apply(lambda x: '<start> ' + x + ' <end>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430d22cd-dfa7-4556-e8ec-fed122f8a5cd",
        "id": "2hG_70vsi10R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2bf8ef1-9af7-419e-e61e-bc8b51f7479b",
        "id": "Hj9pNus8MEk3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function for advanced text cleaning\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstruct the text from tokens\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be6GGhPcSsuI"
      },
      "outputs": [],
      "source": [
        "def clean_text2(text):\n",
        "    # Remove specific unwanted characters\n",
        "    text = text.replace('”', '').replace('–', '')\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jykjN0dWSzP2"
      },
      "outputs": [],
      "source": [
        "data['webTitle'] = data['webTitle'].apply(clean_text2)\n",
        "data['bodyContent'] = data['bodyContent'].apply(clean_text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t3rK5K3vKhZ"
      },
      "outputs": [],
      "source": [
        "# Initialize the tokenizer without specifying `num_words`\n",
        "full_tokenizer = Tokenizer()\n",
        "full_tokenizer.fit_on_texts(data['webTitleToken'])\n",
        "full_tokenizer.fit_on_texts(data['bodyContent'])\n",
        "\n",
        "# Get word counts\n",
        "word_counts = full_tokenizer.word_counts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['webTitle']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577ae6b3-f0bc-4362-dabb-9752f4009aef",
        "id": "OfxZOk8Fi10R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118646    <start> demand tory mp scrap covid rule could ...\n",
              "124284    <start> really smart motorway would lower spee...\n",
              "141061    <start> ‘profiting suffering’ ap cancel sale m...\n",
              "80327     <start> school prison pipeline criminal justic...\n",
              "96867     <start> naked coronavirus tale desperation mad...\n",
              "                                ...                        \n",
              "140283    <start> uk new zealand sign free trade deal <end>\n",
              "110667    <start> crisis stage texas border city reel co...\n",
              "1812      <start> apple iphone sale projected stagnant q...\n",
              "91415      <start> woke gammon buzzword people coined <end>\n",
              "31590     <start> arsène wenger say arsenal fa cup win a...\n",
              "Name: webTitle, Length: 1486, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvDjbIqmvKVp",
        "outputId": "661073b5-1d90-4f17-cb2c-dfb63a952ede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 50 words by frequency: [('said', 486633), ('year', 334501), ('one', 304523), ('would', 271898), ('people', 263636), ('time', 240050), ('say', 212599), ('new', 197682), ('also', 193549), ('like', 184217), ('u', 180507), ('it’s', 169997), ('first', 169517), ('could', 156848), ('two', 153007), ('starttoken', 148695), ('endtoken', 148695), ('day', 144839), ('last', 143214), ('government', 138077), ('make', 128992), ('get', 126799), ('way', 122873), ('work', 121489), ('back', 121474), ('many', 117643), ('world', 113354), ('even', 112328), ('life', 111332), ('week', 109845), ('right', 106036), ('uk', 104378), ('may', 103645), ('made', 103183), ('home', 102982), ('need', 101490), ('much', 99155), ('take', 98164), ('still', 97333), ('go', 97309), ('“i', 95846), ('country', 93366), ('thing', 92898), ('well', 91119), ('woman', 90865), ('three', 90595), ('come', 90131), ('think', 88631), ('don’t', 87589), ('“the', 87567)]\n"
          ]
        }
      ],
      "source": [
        "# Convert the word_counts to a list of (word, count) tuples\n",
        "word_count_list = [(word, count) for word, count in word_counts.items()]\n",
        "\n",
        "# Sort the word count list based on counts (descending)\n",
        "word_count_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the top 50 words\n",
        "print(\"Top 50 words by frequency:\", word_count_list[:50])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqL4juyqvTPM",
        "outputId": "787428a1-839e-4956-f175-918ce5b40ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 50 words by frequency: [('said', 486633), ('year', 334501), ('one', 304523), ('would', 271898), ('people', 263636), ('time', 240050), ('say', 212599), ('new', 197682), ('also', 193549), ('like', 184217), ('u', 180507), ('it’s', 169997), ('first', 169517), ('could', 156848), ('two', 153007), ('starttoken', 148695), ('endtoken', 148695), ('day', 144839), ('last', 143214), ('government', 138077), ('make', 128992), ('get', 126799), ('way', 122873), ('work', 121489), ('back', 121474), ('many', 117643), ('world', 113354), ('even', 112328), ('life', 111332), ('week', 109845), ('right', 106036), ('uk', 104378), ('may', 103645), ('made', 103183), ('home', 102982), ('need', 101490), ('much', 99155), ('take', 98164), ('still', 97333), ('go', 97309), ('“i', 95846), ('country', 93366), ('thing', 92898), ('well', 91119), ('woman', 90865), ('three', 90595), ('come', 90131), ('think', 88631), ('don’t', 87589), ('“the', 87567)]\n"
          ]
        }
      ],
      "source": [
        "# Convert the word_counts to a list of (word, count) tuples\n",
        "word_count_list = [(word, count) for word, count in word_counts.items()]\n",
        "\n",
        "# Sort the word count list based on counts (descending)\n",
        "word_count_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the top 50 words\n",
        "print(\"Top 50 words by frequency:\", word_count_list[:50])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpoOXYT_vWgs",
        "outputId": "4787d3b9-fa5f-4da9-b529-abe43930e952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Suggested vocabulary size: 29177\n"
          ]
        }
      ],
      "source": [
        "# Calculate cumulative word frequency\n",
        "cumulative_frequency = np.cumsum([count for word, count in word_count_list])\n",
        "\n",
        "# Total number of words\n",
        "total_words = cumulative_frequency[-1]\n",
        "\n",
        "# Determine a threshold (e.g., 95% of the total frequency)\n",
        "threshold = total_words * 0.95\n",
        "\n",
        "# Find the vocabulary size needed to reach this threshold\n",
        "vocab_size = next(i for i, total in enumerate(cumulative_frequency) if total > threshold)\n",
        "\n",
        "print(\"Suggested vocabulary size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b4a451-7003-4427-ac4e-5541390f3857",
        "id": "A46L5XwdMnRL"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         starttoken half british adult visited church p...\n",
              "1         starttoken war peace recap episode five – hero...\n",
              "2         starttoken angelique kerber aim dislodge seren...\n",
              "3         starttoken fulham call moussa dembéle’s £m mov...\n",
              "4         starttoken northampton kieran brooke drafted e...\n",
              "                                ...                        \n",
              "148719    starttoken marble head hercules pulled roman s...\n",
              "148720    starttoken ‘i got sick talking myself’ spacey ...\n",
              "148721    starttoken small town big potato inspired glob...\n",
              "148722    starttoken power ban citizen entering australi...\n",
              "148723    starttoken liberal mp say peter dutton let par...\n",
              "Name: webTitleToken, Length: 148695, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data['webTitleToken']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer with a smaller vocabulary\n",
        "max_vocab_size = 50  # Adjust as needed\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
        "tokenizer.fit_on_texts(data['webTitle'])\n",
        "tokenizer.fit_on_texts(data['bodyContent'])\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences_body = tokenizer.texts_to_sequences(data['bodyContent'])\n",
        "sequences_title = tokenizer.texts_to_sequences(data['webTitle'])\n",
        "\n",
        "# Padding sequences\n",
        "body_padded = pad_sequences(sequences_body, maxlen=300)  # Adjust maxlen as per your data\n",
        "title_padded = pad_sequences(sequences_title, maxlen=30, padding='post')  # Adjust maxlen as per your data\n",
        "\n",
        "# Preparing decoder input\n",
        "title_padded_shifted = np.zeros_like(title_padded)\n",
        "title_padded_shifted[:, 1:] = title_padded[:, :-1]"
      ],
      "metadata": {
        "id": "8Uyzmusni10R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "661073b5-1d90-4f17-cb2c-dfb63a952ede",
        "id": "gYp16aSnNJY3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 50 words by frequency: [('said', 486633), ('year', 334501), ('one', 304523), ('would', 271898), ('people', 263636), ('time', 240050), ('say', 212599), ('new', 197682), ('also', 193549), ('like', 184217), ('u', 180507), ('it’s', 169997), ('first', 169517), ('could', 156848), ('two', 153007), ('starttoken', 148695), ('endtoken', 148695), ('day', 144839), ('last', 143214), ('government', 138077), ('make', 128992), ('get', 126799), ('way', 122873), ('work', 121489), ('back', 121474), ('many', 117643), ('world', 113354), ('even', 112328), ('life', 111332), ('week', 109845), ('right', 106036), ('uk', 104378), ('may', 103645), ('made', 103183), ('home', 102982), ('need', 101490), ('much', 99155), ('take', 98164), ('still', 97333), ('go', 97309), ('“i', 95846), ('country', 93366), ('thing', 92898), ('well', 91119), ('woman', 90865), ('three', 90595), ('come', 90131), ('think', 88631), ('don’t', 87589), ('“the', 87567)]\n"
          ]
        }
      ],
      "source": [
        "# Convert the word_counts to a list of (word, count) tuples\n",
        "word_count_list = [(word, count) for word, count in word_counts.items()]\n",
        "\n",
        "# Sort the word count list based on counts (descending)\n",
        "word_count_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the top 50 words\n",
        "print(\"Top 50 words by frequency:\", word_count_list[:50])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787428a1-839e-4956-f175-918ce5b40ef4",
        "id": "cocEDb9ZNJZA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 50 words by frequency: [('said', 486633), ('year', 334501), ('one', 304523), ('would', 271898), ('people', 263636), ('time', 240050), ('say', 212599), ('new', 197682), ('also', 193549), ('like', 184217), ('u', 180507), ('it’s', 169997), ('first', 169517), ('could', 156848), ('two', 153007), ('starttoken', 148695), ('endtoken', 148695), ('day', 144839), ('last', 143214), ('government', 138077), ('make', 128992), ('get', 126799), ('way', 122873), ('work', 121489), ('back', 121474), ('many', 117643), ('world', 113354), ('even', 112328), ('life', 111332), ('week', 109845), ('right', 106036), ('uk', 104378), ('may', 103645), ('made', 103183), ('home', 102982), ('need', 101490), ('much', 99155), ('take', 98164), ('still', 97333), ('go', 97309), ('“i', 95846), ('country', 93366), ('thing', 92898), ('well', 91119), ('woman', 90865), ('three', 90595), ('come', 90131), ('think', 88631), ('don’t', 87589), ('“the', 87567)]\n"
          ]
        }
      ],
      "source": [
        "# Convert the word_counts to a list of (word, count) tuples\n",
        "word_count_list = [(word, count) for word, count in word_counts.items()]\n",
        "\n",
        "# Sort the word count list based on counts (descending)\n",
        "word_count_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the top 50 words\n",
        "print(\"Top 50 words by frequency:\", word_count_list[:50])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4787d3b9-fa5f-4da9-b529-abe43930e952",
        "id": "H9xPJp08NJZA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Suggested vocabulary size: 29177\n"
          ]
        }
      ],
      "source": [
        "# Calculate cumulative word frequency\n",
        "cumulative_frequency = np.cumsum([count for word, count in word_count_list])\n",
        "\n",
        "# Total number of words\n",
        "total_words = cumulative_frequency[-1]\n",
        "\n",
        "# Determine a threshold (e.g., 95% of the total frequency)\n",
        "threshold = total_words * 0.95\n",
        "\n",
        "# Find the vocabulary size needed to reach this threshold\n",
        "vocab_size = next(i for i, total in enumerate(cumulative_frequency) if total > threshold)\n",
        "\n",
        "print(\"Suggested vocabulary size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b4a451-7003-4427-ac4e-5541390f3857",
        "id": "1fUl7yNDNJZA"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         starttoken half british adult visited church p...\n",
              "1         starttoken war peace recap episode five – hero...\n",
              "2         starttoken angelique kerber aim dislodge seren...\n",
              "3         starttoken fulham call moussa dembéle’s £m mov...\n",
              "4         starttoken northampton kieran brooke drafted e...\n",
              "                                ...                        \n",
              "148719    starttoken marble head hercules pulled roman s...\n",
              "148720    starttoken ‘i got sick talking myself’ spacey ...\n",
              "148721    starttoken small town big potato inspired glob...\n",
              "148722    starttoken power ban citizen entering australi...\n",
              "148723    starttoken liberal mp say peter dutton let par...\n",
              "Name: webTitleToken, Length: 148695, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data['webTitleToken']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiPa7OLOME6J"
      },
      "source": [
        "This code initializes a tokenizer with a specified maximum vocabulary size and fits it on the text data. It creates a Tokenizer object with a vocabulary limit (max_vocab_size) set to 50. The tokenizer is then trained on the 'webTitle' and 'bodyContent' text data from the dataset. After fitting, it checks if the special token 'starttoken' is part of the tokenizer's word index. This is essential for sequence-to-sequence models in NLP, as it indicates whether the special tokens used to denote the start of sequences are recognized by the tokenizer. The output will confirm if 'starttoken' is included in the tokenizer's vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2d2fd4-1421-4bef-ef74-8bc568aa252e",
        "id": "N1Ff4SFbNJZB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'starttoken' token is in the tokenizer's word index.\n"
          ]
        }
      ],
      "source": [
        "# Initialize tokenizer with a smaller vocabulary\n",
        "max_vocab_size = 30000  # Adjust as needed\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
        "tokenizer.fit_on_texts(data['webTitleToken'])\n",
        "tokenizer.fit_on_texts(data['bodyContent'])\n",
        "\n",
        "# Check if 'starttoken' token is in the tokenizer's word index\n",
        "if 'starttoken' in tokenizer.word_index:\n",
        "    print(\"'starttoken' token is in the tokenizer's word index.\")\n",
        "else:\n",
        "    print(\"'starttoken' token is NOT in the tokenizer's word index.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE8mewUZMb-b"
      },
      "source": [
        "This code block converts text data into sequences of integers and pads them for consistent length, preparing them for input into a neural network. It first transforms the text in 'bodyContent' and 'webTitle' columns into sequences of integers using the previously trained tokenizer. Then, it pads these sequences to ensure they all have the same length: 300 for the body content and 30 for the titles. This uniform length is necessary for batch processing in neural networks. Additionally, a shifted version of the title sequences is created, which is commonly used as input to the decoder in sequence-to-sequence models, facilitating the prediction of the next word in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml2MuMGopZI_"
      },
      "outputs": [],
      "source": [
        "# Convert texts to sequences\n",
        "sequences_body = tokenizer.texts_to_sequences(data['bodyContent'])\n",
        "sequences_title = tokenizer.texts_to_sequences(data['webTitleToken'])\n",
        "\n",
        "# Padding sequences\n",
        "body_padded = pad_sequences(sequences_body, maxlen=500)  # Adjust maxlen as per your data\n",
        "title_padded = pad_sequences(sequences_title, maxlen=30, padding='post')  # Adjust maxlen as per your data\n",
        "\n",
        "# Preparing decoder input\n",
        "title_padded_shifted = np.zeros_like(title_padded)\n",
        "title_padded_shifted[:, 1:] = title_padded[:, :-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwzjvmXjMp-_"
      },
      "source": [
        "This code block saves the trained tokenizer to a file for future use. It utilizes the pickle module to serialize the tokenizer object and writes it to a file named 'tokenizer.pickle' in a specified directory on Google Drive. The pickle.HIGHEST_PROTOCOL is used for efficient serialization. Saving the tokenizer is important because it ensures consistency in tokenization when preprocessing new data for predictions, especially if the model is to be used or deployed later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7o8ytg5Jfq6"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, load_model\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/path/Models/seq2seq_headline_generator/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into training and validation sets\n",
        "X_train, X_val, title_train, title_val, decoder_input_train, decoder_input_val = train_test_split(\n",
        "    body_padded, title_padded, title_padded_shifted, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "1Y5ZDc6Ui10R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert to sparse categorical\n",
        "def to_sparse_categorical(y, num_classes):\n",
        "    y_sparse = []\n",
        "    for sequence in y:\n",
        "        y_sparse_seq = np.zeros((len(sequence), num_classes))\n",
        "        for i, idx in enumerate(sequence):\n",
        "            if idx < num_classes:\n",
        "                y_sparse_seq[i, idx] = 1\n",
        "        y_sparse.append(y_sparse_seq)\n",
        "    return np.array(y_sparse)\n",
        "\n",
        "# Converting training and validation target data to sparse categorical\n",
        "num_classes = max_vocab_size + 1  # +1 for the padding token\n",
        "y_train_sparse = to_sparse_categorical(title_train, num_classes)\n",
        "y_val_sparse = to_sparse_categorical(title_val, num_classes)"
      ],
      "metadata": {
        "id": "H3eR1eb5MKoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6MuGBUxM23-"
      },
      "source": [
        "This code block is responsible for splitting the padded sequences into training and validation sets. It uses the train_test_split function from the sklearn.model_selection module to divide the dataset into two parts: one for training the model (X_train, title_train, decoder_input_train) and the other for validating the model's performance (X_val, title_val, decoder_input_val). The test_size=0.2 parameter specifies that 20% of the data should be reserved for the validation set, while the remaining 80% is used for training. The random_state=42 ensures that the split is reproducible, meaning the same split will occur each time the code is run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omYP1avfN2Jf"
      },
      "outputs": [],
      "source": [
        "# Splitting data into training and validation sets\n",
        "X_train, X_val, title_train, title_val, decoder_input_train, decoder_input_val = train_test_split(\n",
        "    body_padded, title_padded, title_padded_shifted, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwJtQd95NHAj"
      },
      "source": [
        "This code block includes a function to convert sequence data into sparse categorical format and applies this conversion to the training and validation sets. The function to_sparse_categorical takes each sequence (y) and the number of classes (num_classes) as inputs. It creates a sparse matrix where each row corresponds to a sequence, and each column represents a class (word in the vocabulary). For each word index in the sequence, it sets the corresponding column to 1, resulting in a one-hot encoded representation of the sequences. This sparse format is particularly useful for handling large vocabularies in classification tasks. The function is then used to convert both the training (title_train) and validation (title_val) target data into this sparse categorical format, facilitating their use in training a neural network model for sequence prediction tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykttfv1cd9CS"
      },
      "outputs": [],
      "source": [
        "max_features = max_vocab_size + 1  # +1 for padding token\n",
        "embedding_dim = 128"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Bidirectional\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(input_dim=max_vocab_size + 1, output_dim=128)\n",
        "encoder_lstm = Bidirectional(LSTM(128, return_state=True))\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding_layer = Embedding(input_dim=max_vocab_size + 1, output_dim=128)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_dense = Dense(max_vocab_size + 1, activation='softmax')  # Correct number of units\n",
        "\n",
        "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Seq2Seq Model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n"
      ],
      "metadata": {
        "id": "pa65WJMqBu9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "PV8zb5aEDYoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akjbd0XaNhvT"
      },
      "source": [
        "Model Overview:\n",
        "\n",
        "This code sets up a sequence-to-sequence (Seq2Seq) neural network model, commonly used in natural language processing tasks like machine translation or text summarization.\n",
        "The model consists of two main components: an encoder and a decoder, both utilizing Long Short-Term Memory (LSTM) networks, which are effective in handling sequential data.\n",
        "Encoder:\n",
        "\n",
        "Input Layer: encoder_inputs is the input layer, accepting sequences of indefinite length.\n",
        "Embedding Layer: Transforms the input sequences into dense vectors of fixed size (128 dimensions in this case).\n",
        "Dropout Layer: Applied after the embedding layer to prevent overfitting by randomly setting a fraction of the input units to 0.\n",
        "Stacked LSTM Layers: Two bidirectional LSTM layers (encoder_lstm and encoder_lstm_2) are used. The first LSTM layer returns sequences, while the second returns only the final states. These layers help the model to capture information from both directions of the input sequence (forward and backward).\n",
        "States Concatenation: The final hidden and cell states (state_h and state_c) are concatenated to form the complete state representation from both LSTMs.\n",
        "Decoder:\n",
        "\n",
        "Input Layer: decoder_inputs is the input layer for the decoder.\n",
        "Embedding Layer: Similar to the encoder, it converts the decoder input to dense vectors.\n",
        "Dropout Layer: Applied to the decoder's embedding output.\n",
        "LSTM Layer: A single LSTM layer with a higher number of units (1024) processes the input sequence, utilizing the encoder's final states as its initial states.\n",
        "Dense Layers: Two Dense layers are added. The first (decoder_dense_1) with 512 units and ReLU activation introduces non-linearity and complexity. The second (decoder_dense_2) with softmax activation outputs a probability distribution over the vocabulary for each output sequence element.\n",
        "Seq2Seq Model Construction:\n",
        "\n",
        "The model is constructed by connecting the encoder and decoder components.\n",
        "The encoder's output states are fed into the decoder LSTM as initial states, facilitating the flow of information from the encoder to the decoder.\n",
        "Compilation:\n",
        "\n",
        "The model is compiled with the Adam optimizer and categorical cross-entropy loss, which is standard for multi-class classification problems like this.\n",
        "Key Aspects:\n",
        "\n",
        "The use of bidirectional LSTMs in the encoder enables the model to capture context from both directions, improving its understanding of the input sequence.\n",
        "The increased complexity in the decoder, with additional LSTM units and Dense layers, allows the model to generate more nuanced and accurate output sequences.\n",
        "Dropout layers aid in reducing overfitting, especially crucial in a model with high complexity and many parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu-tjj-VOhai"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Bidirectional, Dropout\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(input_dim=max_vocab_size + 1, output_dim=128)(encoder_inputs)\n",
        "encoder_dropout = Dropout(0.5)(encoder_embedding)  # Dropout layer after embedding\n",
        "# Stacking LSTMs in the encoder\n",
        "encoder_lstm = Bidirectional(LSTM(512, return_sequences=True, return_state=True))\n",
        "encoder_lstm_2 = Bidirectional(LSTM(512, return_state=True))\n",
        "\n",
        "# Applying the first LSTM and then the second\n",
        "encoder_output, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_dropout)\n",
        "_, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_lstm_2(encoder_output)\n",
        "\n",
        "# Using the states from the last LSTM layer\n",
        "state_h = Concatenate()([forward_h2, backward_h2])\n",
        "state_c = Concatenate()([forward_c2, backward_c2])\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Adjust the size of the states to match the decoder LSTM's expected state size\n",
        "state_h = Dense(1024, activation='relu')(state_h)  # Reduce size to 1024\n",
        "state_c = Dense(1024, activation='relu')(state_c)  # Reduce size to 1024\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding_layer = Embedding(input_dim=max_vocab_size + 1, output_dim=128)\n",
        "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
        "decoder_dropout = Dropout(0.5)(decoder_embedding)  # Dropout layer after embedding\n",
        "decoder_lstm = LSTM(1024, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_dropout, initial_state=encoder_states)\n",
        "\n",
        "# Adding the new Dense layers\n",
        "decoder_dense_1 = Dense(1024, activation='relu')  # New intermediate Dense layer\n",
        "decoder_intermediate = decoder_dense_1(decoder_outputs)  # Apply the new Dense layer\n",
        "decoder_dense_2 = Dense(max_vocab_size + 1, activation='softmax')  # Final Dense layer for output\n",
        "decoder_outputs = decoder_dense_2(decoder_intermediate)  # Apply the final Dense layer\n",
        "\n",
        "# Seq2Seq Model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67e38bc1-5f46-4100-b094-14bd266840c8",
        "id": "LHuGURluOhaj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, None, 128)            3840128   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, None, 128)            0         ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " bidirectional (Bidirection  [(None, None, 1024),         2625536   ['dropout[0][0]']             \n",
            " al)                          (None, 512),                                                        \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512)]                                                        \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirecti  [(None, 1024),               6295552   ['bidirectional[0][0]']       \n",
            " onal)                        (None, 512),                                                        \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512)]                                                        \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, None, 128)            3840128   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 1024)                 0         ['bidirectional_1[0][1]',     \n",
            "                                                                     'bidirectional_1[0][3]']     \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 1024)                 0         ['bidirectional_1[0][2]',     \n",
            " )                                                                   'bidirectional_1[0][4]']     \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, None, 128)            0         ['embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 1024)                 1049600   ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1024)                 1049600   ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)               [(None, None, 1024),         4722688   ['dropout_1[0][0]',           \n",
            "                              (None, 1024),                          'dense[0][0]',               \n",
            "                              (None, 1024)]                          'dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, None, 1024)           1049600   ['lstm_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, None, 30001)          3075102   ['dense_2[0][0]']             \n",
            "                                                          5                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 55223857 (210.66 MB)\n",
            "Trainable params: 55223857 (210.66 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([X_train, decoder_input_train], y_train_sparse,\n",
        "                    validation_data=([X_val, decoder_input_val], y_val_sparse),\n",
        "                    epochs=5,  # Adjust as needed\n",
        "                    batch_size=32)"
      ],
      "metadata": {
        "id": "85csh9VyBu6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, load_model\n",
        "import pickle\n",
        "#saving model\n",
        "model.save('/content/drive/MyDrive/path/Models/seq2seq_headline_generator/seq2seq_headline_generator.h5')\n",
        "with open('/content/drive/My Drive/path/Models/seq2seq_headline_generator/history.p', 'wb') as f:\n",
        "    pickle.dump(history.history, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9EsR7nAizNj",
        "outputId": "c5f63c55-828b-4124-d308-801b5b62d028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwA6GNUON11o"
      },
      "source": [
        "This code block is for training the Seq2Seq model with early stopping. It trains the model on the training data and validates it on the validation data, stopping the training early if there's no improvement in validation loss for two consecutive epochs. This approach helps in preventing overfitting and ensures efficient training. The model is trained for a maximum of 10 epochs with a batch size of 64, and the training progress is monitored using the EarlyStopping callback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69bed646-3084-4c79-f379-afb412319383",
        "id": "kYREbl-CPYJP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "930/930 [==============================] - 745s 801ms/step - loss: 2.2019 - val_loss: 2.1414\n",
            "Epoch 2/50\n",
            "930/930 [==============================] - 718s 772ms/step - loss: 2.0811 - val_loss: 2.0564\n",
            "Epoch 3/50\n",
            "930/930 [==============================] - 710s 764ms/step - loss: 1.9749 - val_loss: 1.9826\n",
            "Epoch 4/50\n",
            "930/930 [==============================] - 706s 760ms/step - loss: 1.8827 - val_loss: 1.9502\n",
            "Epoch 5/50\n",
            "930/930 [==============================] - 702s 754ms/step - loss: 1.8054 - val_loss: 1.9416\n",
            "Epoch 6/50\n",
            "930/930 [==============================] - 701s 754ms/step - loss: 1.7327 - val_loss: 1.9506\n",
            "Epoch 7/50\n",
            "930/930 [==============================] - 699s 752ms/step - loss: 1.6568 - val_loss: 1.9829\n",
            "Epoch 8/50\n",
            "930/930 [==============================] - 700s 753ms/step - loss: 1.5830 - val_loss: 2.0192\n",
            "Epoch 9/50\n",
            "930/930 [==============================] - 698s 750ms/step - loss: 1.5137 - val_loss: 2.0799\n",
            "Epoch 10/50\n",
            "930/930 [==============================] - 698s 751ms/step - loss: 1.4502 - val_loss: 2.1432\n",
            "Epoch 11/50\n",
            "930/930 [==============================] - 698s 751ms/step - loss: 1.3926 - val_loss: 2.2241\n",
            "Epoch 12/50\n",
            "930/930 [==============================] - 698s 751ms/step - loss: 1.3422 - val_loss: 2.2874\n",
            "Epoch 13/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 1.2943 - val_loss: 2.3620\n",
            "Epoch 14/50\n",
            "930/930 [==============================] - 698s 751ms/step - loss: 1.2505 - val_loss: 2.4204\n",
            "Epoch 15/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 1.2110 - val_loss: 2.4993\n",
            "Epoch 16/50\n",
            "930/930 [==============================] - 698s 751ms/step - loss: 1.1751 - val_loss: 2.5525\n",
            "Epoch 17/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 1.1422 - val_loss: 2.6075\n",
            "Epoch 18/50\n",
            "930/930 [==============================] - 698s 750ms/step - loss: 1.1106 - val_loss: 2.6857\n",
            "Epoch 19/50\n",
            "930/930 [==============================] - 698s 751ms/step - loss: 1.0815 - val_loss: 2.7298\n",
            "Epoch 20/50\n",
            "930/930 [==============================] - 698s 751ms/step - loss: 1.0563 - val_loss: 2.7845\n",
            "Epoch 21/50\n",
            "930/930 [==============================] - 697s 749ms/step - loss: 1.0297 - val_loss: 2.8330\n",
            "Epoch 22/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 1.0056 - val_loss: 2.8940\n",
            "Epoch 23/50\n",
            "930/930 [==============================] - 697s 749ms/step - loss: 0.9838 - val_loss: 2.9372\n",
            "Epoch 24/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 0.9627 - val_loss: 2.9872\n",
            "Epoch 25/50\n",
            "930/930 [==============================] - 697s 749ms/step - loss: 0.9444 - val_loss: 3.0373\n",
            "Epoch 26/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 0.9246 - val_loss: 3.0898\n",
            "Epoch 27/50\n",
            "930/930 [==============================] - 697s 749ms/step - loss: 0.9066 - val_loss: 3.1179\n",
            "Epoch 28/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 0.8902 - val_loss: 3.1794\n",
            "Epoch 29/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 0.8744 - val_loss: 3.2114\n",
            "Epoch 30/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 0.8592 - val_loss: 3.2684\n",
            "Epoch 31/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 0.8455 - val_loss: 3.2942\n",
            "Epoch 32/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 0.8308 - val_loss: 3.3632\n",
            "Epoch 33/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 0.8180 - val_loss: 3.3944\n",
            "Epoch 34/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 0.8061 - val_loss: 3.4337\n",
            "Epoch 35/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 0.7941 - val_loss: 3.4573\n",
            "Epoch 36/50\n",
            "930/930 [==============================] - 698s 750ms/step - loss: 0.7829 - val_loss: 3.4872\n",
            "Epoch 37/50\n",
            "930/930 [==============================] - 697s 749ms/step - loss: 0.7720 - val_loss: 3.5583\n",
            "Epoch 38/50\n",
            "930/930 [==============================] - 697s 750ms/step - loss: 0.7613 - val_loss: 3.5815\n",
            "Epoch 39/50\n",
            "930/930 [==============================] - 697s 749ms/step - loss: 0.7514 - val_loss: 3.6017\n",
            "Epoch 40/50\n",
            "930/930 [==============================] - 698s 750ms/step - loss: 0.7417 - val_loss: 3.6356\n",
            "Epoch 41/50\n",
            "930/930 [==============================] - 696s 749ms/step - loss: 0.7325 - val_loss: 3.6699\n",
            "Epoch 42/50\n",
            "930/930 [==============================] - 697s 749ms/step - loss: 0.7238 - val_loss: 3.7143\n",
            "Epoch 43/50\n",
            "930/930 [==============================] - 697s 749ms/step - loss: 0.7153 - val_loss: 3.7354\n",
            "Epoch 44/50\n",
            "930/930 [==============================] - 697s 749ms/step - loss: 0.7065 - val_loss: 3.7784\n",
            "Epoch 45/50\n",
            "930/930 [==============================] - 696s 749ms/step - loss: 0.6989 - val_loss: 3.8251\n",
            "Epoch 46/50\n",
            "930/930 [==============================] - 697s 749ms/step - loss: 0.6920 - val_loss: 3.8486\n",
            "Epoch 47/50\n",
            "930/930 [==============================] - 697s 749ms/step - loss: 0.6839 - val_loss: 3.8851\n",
            "Epoch 48/50\n",
            "930/930 [==============================] - 697s 749ms/step - loss: 0.6764 - val_loss: 3.9184\n",
            "Epoch 49/50\n",
            "930/930 [==============================] - 696s 748ms/step - loss: 0.6697 - val_loss: 3.9610\n",
            "Epoch 50/50\n",
            "930/930 [==============================] - 697s 749ms/step - loss: 0.6629 - val_loss: 3.9618\n"
          ]
        }
      ],
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit([X_train, decoder_input_train], title_train,\n",
        "                    validation_data=([X_val, decoder_input_val], title_val),\n",
        "                    epochs=50,  # Adjust epochs as needed\n",
        "                    batch_size=128)  # Adjust batch size as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if4XoNRwODGw"
      },
      "source": [
        "This code block saves the trained Seq2Seq model and its training history. The model is saved as an H5 file (seq2seq_headline_generator.h5) to a specified path on Google Drive. Additionally, the training history, which includes metrics like loss and accuracy for each epoch, is saved as a pickle file (history.p). This allows for later analysis of the model's training process and easy reloading of the model for further use or evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21894655-2f18-41a2-98bc-c9f75b00c3aa",
        "id": "qokaOCO5QO2T"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential, load_model\n",
        "import pickle\n",
        "#saving model\n",
        "model.save('/content/drive/MyDrive/path/Models/seq2seq_headline_generator/seq2seq_headline_generator.h5')\n",
        "with open('/content/drive/My Drive/path/Models/seq2seq_headline_generator/history.p', 'wb') as f:\n",
        "    pickle.dump(history.history, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThTyfuRkOQZV"
      },
      "source": [
        "This code block is for loading the previously trained Seq2Seq model and the associated tokenizer. The load_model function from Keras is used to load the saved model file (seq2seq_headline_generator.h5) from Google Drive. Additionally, the tokenizer, which is essential for processing text inputs in the same format as the training data, is loaded from a saved pickle file (tokenizer.pickle) using the pickle module. This setup is crucial for making new predictions or continuing the model's evaluation using the same tokenization scheme as used during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1wvPfTgslCA"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model, Model\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/path/Models/seq2seq_headline_generator/seq2seq_headline_generator.h5')\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('/content/drive/MyDrive/path/Models/seq2seq_headline_generator/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21894655-2f18-41a2-98bc-c9f75b00c3aa",
        "id": "VlGwXfSaQz1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential, load_model\n",
        "import pickle\n",
        "#saving model\n",
        "model.save('/content/drive/MyDrive/path/Models/seq2seq_headline_generator/seq2seq_headline_generator.h5')\n",
        "with open('/content/drive/My Drive/path/Models/seq2seq_headline_generator/history.p', 'wb') as f:\n",
        "    pickle.dump(history.history, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDvVFiOtQz1m"
      },
      "source": [
        "This code block is for loading the previously trained Seq2Seq model and the associated tokenizer. The load_model function from Keras is used to load the saved model file (seq2seq_headline_generator.h5) from Google Drive. Additionally, the tokenizer, which is essential for processing text inputs in the same format as the training data, is loaded from a saved pickle file (tokenizer.pickle) using the pickle module. This setup is crucial for making new predictions or continuing the model's evaluation using the same tokenization scheme as used during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlJAfUVdQz1m"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model, Model\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/path/Models/seq2seq_headline_generator/seq2seq_headline_generator.h5')\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('/content/drive/MyDrive/path/Models/seq2seq_headline_generator/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jRr_t5zObL3"
      },
      "source": [
        "This code block creates separate models for the encoder and decoder parts of the previously trained Seq2Seq model, which is useful for making predictions. The encoder model is defined to take the original encoder inputs and output the encoder states. For the decoder model, new input layers are created to receive the hidden and cell states, and it is set up to take these states along with the decoder inputs. The decoder model then outputs the predicted sequences and the updated states. This separation into encoder and decoder models is particularly useful for generating sequences in an iterative manner, where the state outputs from one time step are fed back as inputs for the next time step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gAcXIVkN5Fc"
      },
      "outputs": [],
      "source": [
        "# Encoder Model\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "\n",
        "# Inputs for the decoder model\n",
        "decoder_state_input_h = Input(shape=(1024,))\n",
        "decoder_state_input_c = Input(shape=(1024,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_inputs_single = Input(shape=(1,))  # Single timestep input\n",
        "\n",
        "# Decoder setup\n",
        "decoder_state_input_h = Input(shape=(1024,))  # Keep the state size as 1024\n",
        "decoder_state_input_c = Input(shape=(1024,))  # Keep the state size as 1024\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_embedding_single = decoder_embedding_layer(decoder_inputs_single)\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding_single, initial_state=decoder_states_inputs)\n",
        "\n",
        "decoder_outputs = decoder_dense_2(decoder_outputs)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs_single] + decoder_states_inputs,\n",
        "    [decoder_outputs, state_h, state_c]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kL5Lz9GOd_K"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M37TUJosu3CT"
      },
      "outputs": [],
      "source": [
        "input_text = \"Spoiler alert: this recap is for viewers of Deutschland 83 on Channel 4 and Walter Presents, please refrain from posting details from later episodes if youâ€™ve see more. Catch up on episode four here Now thatâ€™s more like it. This was easily the best episode so far, a tense and dark hour in which a number of characters made decisions that will have far-reaching effects. Most of them bad, if Iâ€™m any judge. I understand those who feel that Deutschland 83 has teetered too far into the absurd to work as a serious drama, but I also felt that this week addressed many of those issues, giving us some good character development into the bargain.Is Martin still the worst spy in the universe? Quite probably, but again Iâ€™d argue that this is part of the point â€“ as much as a spy drama, itâ€™s a story about young people struggling to make the correct choices while being conspicuously and continually let down by authority figures. What the Wingers seem intent on doing is giving us a world populated by people who are trying desperately to do the right thing, but are repeatedly doomed to do wrong by the time in which they are living. The west Poor old Martin. No sooner had he shed his inhibitions in the ashram (and to Bonnie Tyler, no less) than he found himself called back to duty and ordered to Berlin, ostensibly to help out his sick mother by giving her his kidney. Of course, Martinâ€™s missions are never as simple as that, and thus our hero was really in Berlin as a patsy, a convenient way of ensuring that a bunch of explosives was delivered to the terrorist who wanted them â€“ with devastating effect. The bomb itself really happened and was really the work, as stated, of Carlos the Jackal. As to the mysterious planter of the bomb, Iâ€™m presuming he was another patsy of a kind, an associate of Carlosâ€™s who agreed to plant the bomb at the French cultural centre. Also worth noting: the suggestion that the East German government had some involvement in the West German bombing is not a new one and the case against Carlos was partially built from old Stasi files.Martin wasnâ€™t the only one having a bad time this week, as Alex realised that loveâ€™s young dream was not quite as rosy as he had hoped, a situation that led to him storming off from the Mansion of Manipulation, initially seeking solace in the ashram (like everyone else before him) before turning up and offering his services to the DDR. Itâ€™s rather typical of Tobiasâ€™s luck that, even though Alex rejected both his idea of returning to the army and his protestations of love, he ended up offering to spy for the East Germans without Tischbier having to sully his hands by suggesting it. That man is so smooth that even his failures turn into some sort of success. The east Over in the east, betrayal was the name of the game as Annett, not content with rejecting Thomas, chose to sell out his mobile library at the same time. Oh Annett, I will find this very hard to forgive, although I do wonder exactly what message she wanted to get to Martin. Did she think he wasnâ€™t going to turn up and help his mother? Because if that was the reason she contacted Schweppenstette, then I suspect she will end up regretting it. As for poor Thomas, now my favourite character on this show after his impassioned defence of literature, things donâ€™t look good for him at all. That will teach him to go talking about love and literature to the first innocent-looking blonde he meets. Nor are they looking good for Ingrid, even though a rather battered Martin did manage to make it in time to donate the kidney. Will she survive? A couple of weeks ago I would have said sure, but this show is getting notably darker by the week so, despite Lenoraâ€™s intervention, I would now put her chances at 50-50 at best.Stasi files â€¢ Martin really lost his innocence this week â€“ no sooner had he lectured Tobias about being a killer, than he finds himself murdering a stranger on a railway track. I do wonder how much longer he can square his conscience with the life he is having to lead. â€¢ It was also made clear that Tobias was responsible for Lindaâ€™s death â€“ he may not have driven the car, but thereâ€™s no doubting that he gave the order. â€¢ Still, I was impressed by the way in which Martin used my sisterâ€™s old technique of telling the truth so outlandishly that the other person doesnâ€™t believe you. Got us out of many a sticky situation with our parents, I can tell you. â€¢ Lenora (Maria Schrader) almost revealed a heart this week. Despite all the manipulation, I think she genuinely does love her sister, and she really sold the tension as she waited for Martin to arrive. (And Iâ€™m presuming that she is the reason why Martin made it East relatively fast, despite turning up bloody and battered to the checkpoint). â€¢ The Edel home really isnâ€™t a safe place for a fish. After Renate attempted to kill them off with champagne glasses last week, Ursula went a step further this week, dropping one on the ground as a minor act of revenge against her husband. â€¢ Talking of Ursula, I was amused by the relish with which she devoured Alexâ€™s toast. I understand why sheâ€™s protecting her son, but her commitment to that cause was still wildly entertaining. â€¢ One thing I wasnâ€™t sure of â€“ did Ursula intend Frau Netz to think Alex has Aids or was the secretary jumping to conclusions? â€¢ Regarding Frau Netz â€“ do we also think she might be one of Lenoraâ€™s secretaries, or is that a spy reveal too far? â€¢ I loved the way Yvonneâ€™s ashram mate took a moment to centre himself after Alexâ€™s outburst. Sometimes itâ€™s the small things that work. â€¢ I also continue to be amused by Walter Schweppenstette, despite his capacity for evil â€“ I was particularly taken by his discussion about Worcester sauce. Sorry, but Iâ€™m a sucker for a man who knows his condiments. Song of the week Only two songs again this week, but they were both well used. Iâ€™m giving the edge to Mad World by Tears for Fears just because it made this cover from Donnie Darko â€“ an improbable Christmas No 1 â€“ pop into my head for the first time in years. Quote of the week â€œGo on, give each other back rubs while the world goes up in a mushroom cloudâ€ â€“ Alex almost wins my heart with his impassioned take down of hippies. Weâ€™ve all been there, Alex. So what did you think â€“ do you approve of the increasingly dark tone? Who is in worse trouble, Alex or Thomas? What about Martin? Will Ursula continue to lie to her husband? And will Yvonne ever make it out of that ashram? As ever, all speculation and no spoilers welcome below â€¦\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDbbhhM5Otx7"
      },
      "source": [
        "This line of code applies the previously defined clean_text function to the variable input_text. The function processes the text by converting it to lowercase, removing punctuation and numbers, eliminating extra spaces, and applying lemmatization. The cleaned and processed text is then stored in the variable cleaned_input_text, ready for further processing or feeding into a trained machine learning model. This step is crucial for maintaining consistency in text preprocessing between training and prediction phases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpwOHNQSu2-l"
      },
      "outputs": [],
      "source": [
        "cleaned_input_text = clean_text(input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL4EfBUHOvdP"
      },
      "source": [
        "This code includes a function preprocess_input_text for tokenizing and padding input text, and then applies this function to the cleaned_input_text. The function takes the cleaned text, uses the trained tokenizer to convert it into a sequence of integers, and then pads this sequence to a specified maximum length (50 in this case) to ensure consistency with the model's input requirements. The output, preprocessed_input, is a padded sequence of the input text, ready to be fed into the model for prediction. This preprocessing step is crucial for aligning the format of new input data with the format used during model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyVGnnYqu27S"
      },
      "outputs": [],
      "source": [
        "def preprocess_input_text(text, tokenizer, max_length):\n",
        "    # Tokenize the text\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "\n",
        "    # Pad the sequence\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_length)\n",
        "\n",
        "    return padded_sequence\n",
        "\n",
        "preprocessed_input = preprocess_input_text(cleaned_input_text, tokenizer, 50)  # Use the same max_length as your training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXlIvs-bSROi"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0qeKKOaPBYp"
      },
      "source": [
        "Purpose:\n",
        "\n",
        "This function is designed to generate a headline (text output) from a given input sequence using the trained encoder and decoder models of a Seq2Seq architecture.\n",
        "\n",
        "1.   Encoding Input Sequence:\n",
        "\n",
        "\n",
        "> The input sequence (input_seq) is first processed by the encoder_model to get the state vectors (states_value). These vectors encapsulate the information from the input sequence and serve as the context for the decoder.\n",
        "Initializing Target Sequence:\n",
        "\n",
        "\n",
        "2.   Initializing Target Sequence:\n",
        "\n",
        "> The function initializes a target sequence (target_seq) of length 1 with the index of the 'starttoken'. This serves as the starting point for the decoder to generate the output sequence.\n",
        "\n",
        "3.   Iterative Decoding:\n",
        "\n",
        "> The decoding process is iterative. In each iteration, the decoder model (decoder_model) predicts the next word based on the current target_seq and states_value.\n",
        "The np.argmax function is used to find the index of the most probable next word from the decoder's output (output_tokens). This index is then converted back to the corresponding word using the tokenizer's index_word dictionary.\n",
        "\n",
        "4.   Building the Output Sentence:\n",
        "\n",
        ">The sampled word is added to decoded_sentence unless it is the 'endtoken', which signifies the end of the output sequence.\n",
        "The process continues until either the 'endtoken' is encountered or the length of the decoded sentence reaches the specified max_length.\n",
        "\n",
        "5.   Updating Target Sequence and States:\n",
        "\n",
        "> After each iteration, the target_seq is updated with the index of the last predicted word, and states_value is updated with the current state of the decoder. This setup allows the model to consider both the previously generated words and the input sequence's context for the next word prediction.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "The function returns the generated headline (decoded_sentence), a coherent sequence of words formed by iteratively predicting one word at a time based on the context provided by the input sequence and the previously generated words."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_title(preprocessed_input, encoder_model, decoder_model, tokenizer, max_title_length=30):\n",
        "    # Encode the input as state vectors\n",
        "    states_value = encoder_model.predict(preprocessed_input)\n",
        "\n",
        "    # Generate empty target sequence of length 1 with only the start token\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = tokenizer.word_index.get('starttoken', 1)  # Default to 1 if 'starttoken' not found\n",
        "\n",
        "    stop_condition = False\n",
        "    generated_title = ''\n",
        "    while not stop_condition:\n",
        "        # Predict the next word and the next states\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = tokenizer.index_word.get(sampled_token_index, '')  # Handle unknown indices\n",
        "\n",
        "        # Append the sampled word to the generated title\n",
        "        if sampled_word != 'endtoken':\n",
        "            generated_title += ' ' + sampled_word\n",
        "\n",
        "        # Exit condition: either hit max length or find the end token\n",
        "        if sampled_word == 'endtoken' or len(generated_title.split()) >= max_title_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence and states for the next prediction\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return generated_title.strip()"
      ],
      "metadata": {
        "id": "d8peOcKNHkrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az88OfDTlzNh",
        "outputId": "6df7c852-db13-48fb-c279-163801099a2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1375, 10968, 19008, 14773,   609, 18201,   178,  7450,  1976,\n",
              "            4,    38,  1164,   156,   416,  2016,     5,    89,   687,\n",
              "         9227, 25401,   894, 20011,  8875,   342,  5025,   665,    59,\n",
              "           31, 18082,   285,  1519,   575,  2282,  1701,   285,    33,\n",
              "         5301,   778,  4329,   205,   136,    55,    95,   889,   664,\n",
              "          614,   119,   279,  1713,   676]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "preprocessed_input"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function with the preprocessed input\n",
        "generated_title = generate_title(preprocessed_input, encoder_model, decoder_model, tokenizer)\n",
        "print(\"Generated Title:\", generated_title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-X95ZrGHq17",
        "outputId": "3aea8123-971a-4175-babd-def5825b2a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Generated Title: seems taboo shield mouthed destroyed sandy root coward pool abandoned literally metre slot millennials kidnapped millennials pimp mater stranded ftse abandoned pimp flat stranded period flat period flat period flat\n"
          ]
        }
      ]
    }
  ]
}