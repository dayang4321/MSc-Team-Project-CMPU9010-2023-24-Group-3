{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "385b818c-6f9e-4afc-ac9a-a13c6430bc5a",
        "id": "EZUOmWP4i10P"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from scipy import sparse\n",
        "import string\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(2)\n",
        "from numpy.random import seed\n",
        "seed(1)"
      ],
      "metadata": {
        "id": "1Ow-ztU_i10Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtQbvBxDIq4y"
      },
      "source": [
        "This code block is used to mount your Google Drive to the Colab environment. It allows you to access files stored in your Google Drive directly from the Colab notebook. The force_remount=True parameter ensures that the drive is remounted if it was previously mounted during the session. This is useful for ensuring that the latest version of the drive's contents is accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c72c502-3137-402a-e493-6fafd59f4cc5",
        "id": "VVg6UEsLJ7Mj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TsVvmSjIzYL"
      },
      "source": [
        "This code block imports essential libraries and modules for data processing, machine learning model building, and text analysis. It includes libraries for data manipulation (Pandas), neural network construction (Keras, TensorFlow), natural language processing (NLTK), and various utilities for handling arrays, strings, and file paths. Additionally, it sets the seed for random number generation to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OFsBI1uJ7Mj"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from scipy import sparse\n",
        "import string\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(2)\n",
        "from numpy.random import seed\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgSV__GOoDoR"
      },
      "outputs": [],
      "source": [
        "# data_url = \"something\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iMpIeV-K4AZ"
      },
      "source": [
        "This line of code assigns a URL to the variable data_url. The URL points to a Google Drive folder containing the data files needed for the project. The data can be accessed and downloaded using this URL, typically in combination with Google Drive APIs or tools like gdown to facilitate the download process in a programming environment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"https://drive.google.com/drive/folders/1_RGwmjvW8p3jHuKqIbbHXdBDuDl2DTjU?usp=drive_link\""
      ],
      "metadata": {
        "id": "jc0_olxki10Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder (data_url, quiet=True, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1bec2a4-c37c-4c44-8965-a9f90a797ece",
        "id": "BNe2bDAgi10Q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Guardian News Articles/guardian_articles.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data_sample = pd.read_csv('/content/Guardian News Articles/guardian_articles.csv')\n",
        "\n",
        "# Using only 10% of the data\n",
        "data = data_sample.sample(frac=0.01, random_state=42)\n",
        "\n",
        "# Drop rows with missing values\n",
        "data.dropna(subset=['webTitle', 'bodyContent'], inplace=True)"
      ],
      "metadata": {
        "id": "KRCaOza4i10Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3Ef5JXnLcUf"
      },
      "outputs": [],
      "source": [
        "data_url = \"https://drive.google.com/drive/folders/1_RGwmjvW8p3jHuKqIbbHXdBDuDl2DTjU?usp=drive_link\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfH94Bm6LFI_"
      },
      "source": [
        "This code uses the gdown library to download an entire folder from Google Drive. The data_url specifies the location of the folder on Google Drive. The quiet=True parameter ensures that the download process runs without printing unnecessary logs, and use_cookies=False indicates that cookies should not be used in the download process. This is a convenient way to programmatically download all files from a shared Google Drive folder directly into the runtime environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124707be-bc99-426b-f92b-9c65876452c1",
        "id": "zi5rayOlLcUg"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Guardian News Articles/cleaned_guardian_articles_test.csv',\n",
              " '/content/Guardian News Articles/cleaned_guardian_articles_train.csv',\n",
              " '/content/Guardian News Articles/cleaned_guardian_articles.csv',\n",
              " '/content/Guardian News Articles/guardian_articles.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "gdown.download_folder (data_url, quiet=True, use_cookies=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNwdxLv0LVyC"
      },
      "source": [
        "This code block is responsible for loading and cleaning a dataset. It first reads a CSV file named 'guardian_articles.csv' from the specified directory into a pandas DataFrame called data_sample. After loading the data, it removes rows where values in either the 'webTitle' or 'bodyContent' columns are missing. This cleaning step is crucial to ensure the quality and reliability of the dataset for further processing and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJWQDtPHLcUg"
      },
      "outputs": [],
      "source": [
        "# # Load dataset\n",
        "# data = pd.read_csv('/content/Guardian News Articles/guardian_articles.csv')\n",
        "\n",
        "# # Using only 10% of the data\n",
        "# # data = data_sample.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# # Drop rows with missing values\n",
        "# data.dropna(subset=['webTitle', 'bodyContent'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G4rUCDFLZoG"
      },
      "source": [
        "This code block sets up the necessary components for advanced text preprocessing. It starts by downloading NLTK datasets for stopwords and WordNet, which are used for text cleaning and normalization. A function clean_text is defined to convert text to lowercase, remove punctuation and numbers, eliminate extra spaces, tokenize, remove stopwords, and apply lemmatization. This function is applied to the 'webTitle' and 'bodyContent' columns of the dataset to clean and preprocess the text. Additionally, special tokens 'starttoken' and 'endtoken' are added to each headline in 'webTitle' to signify the beginning and end, which is a common technique in natural language processing, especially in sequence modeling tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buk_6FCP-RK6"
      },
      "outputs": [],
      "source": [
        "# Load the cleaned dataset from a CSV file\n",
        "data = pd.read_csv('/content/Guardian News Articles/cleaned_guardian_articles_train.csv')\n",
        "\n",
        "# Drop rows with missing values\n",
        "data.dropna(subset=['webTitle', 'bodyContent','webTitleToken'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "rD8BLqU3NXlQ",
        "outputId": "9ea9709b-2819-40b4-8c13-167dd410c166"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               article_id         sectionName  \\\n",
              "0       world/2016/jan/31/church-christian-british-adu...          World news   \n",
              "1       tv-and-radio/2016/jan/31/war-and-peace-recap-e...  Television & radio   \n",
              "2       sport/2016/jan/31/angelique-kerber-serena-will...               Sport   \n",
              "3       football/2016/jan/31/fulham-call-off-moussa-de...            Football   \n",
              "4          sport/2016/jan/31/kieran-brookes-england-squad               Sport   \n",
              "...                                                   ...                 ...   \n",
              "148719  world/2022/jun/21/marble-head-of-hercules-pull...          World news   \n",
              "148720  music/2022/jun/22/i-got-sick-of-talking-about-...               Music   \n",
              "148721  australia-news/2022/jun/22/the-small-town-with...      Australia news   \n",
              "148722  australia-news/2022/jun/22/power-to-ban-citize...      Australia news   \n",
              "148723  australia-news/2022/jun/22/liberal-mps-say-pet...      Australia news   \n",
              "\n",
              "                                                 webTitle  \\\n",
              "0             half british adult visited church past year   \n",
              "1       war peace recap episode five – hero leech cast...   \n",
              "2       angelique kerber aim dislodge serena williams ...   \n",
              "3       fulham call moussa dembéle’s £m move tottenham...   \n",
              "4       northampton kieran brooke drafted england squa...   \n",
              "...                                                   ...   \n",
              "148719  marble head hercules pulled roman shipwreck si...   \n",
              "148720  ‘i got sick talking myself’ spacey jane back m...   \n",
              "148721   small town big potato inspired global poetry win   \n",
              "148722  power ban citizen entering australia questione...   \n",
              "148723  liberal mp say peter dutton let party room dec...   \n",
              "\n",
              "                                                   webUrl  \\\n",
              "0       https://www.theguardian.com/world/2016/jan/31/...   \n",
              "1       https://www.theguardian.com/tv-and-radio/2016/...   \n",
              "2       https://www.theguardian.com/sport/2016/jan/31/...   \n",
              "3       https://www.theguardian.com/football/2016/jan/...   \n",
              "4       https://www.theguardian.com/sport/2016/jan/31/...   \n",
              "...                                                   ...   \n",
              "148719  https://www.theguardian.com/world/2022/jun/21/...   \n",
              "148720  https://www.theguardian.com/music/2022/jun/22/...   \n",
              "148721  https://www.theguardian.com/australia-news/202...   \n",
              "148722  https://www.theguardian.com/australia-news/202...   \n",
              "148723  https://www.theguardian.com/australia-news/202...   \n",
              "\n",
              "                                              bodyContent  \\\n",
              "0       half british adult visited church past year de...   \n",
              "1       ‘strange thing turn sometimes …’ pierre get aw...   \n",
              "2       gone midnight angelique kerber conducting yet ...   \n",
              "3       fulham angrily called moussa dembélé’s propose...   \n",
              "4       england drafted northampton prop kieran brooke...   \n",
              "...                                                   ...   \n",
              "148719  archaeologist it’s underwater find keep giving...   \n",
              "148720  mop curl caleb harper – spacey jane frontman g...   \n",
              "148721  robertson small pretty town perched edge new s...   \n",
              "148722  high court decision striking home affair minis...   \n",
              "148723  liberal mp urging peter dutton let party room ...   \n",
              "\n",
              "          webPublicationDate        id  \\\n",
              "0       2016-01-31T22:00:09Z      21.0   \n",
              "1       2016-01-31T22:00:09Z      22.0   \n",
              "2       2016-01-31T21:59:09Z      23.0   \n",
              "3       2016-01-31T21:56:54Z      24.0   \n",
              "4       2016-01-31T21:52:14Z      25.0   \n",
              "...                      ...       ...   \n",
              "148719  2022-06-21T17:31:32Z  149835.0   \n",
              "148720  2022-06-21T17:30:09Z  149836.0   \n",
              "148721  2022-06-21T17:30:09Z  149837.0   \n",
              "148722  2022-06-21T17:30:08Z  149838.0   \n",
              "148723  2022-06-21T17:30:08Z  149839.0   \n",
              "\n",
              "                                            webTitleToken  \n",
              "0       starttoken half british adult visited church p...  \n",
              "1       starttoken war peace recap episode five – hero...  \n",
              "2       starttoken angelique kerber aim dislodge seren...  \n",
              "3       starttoken fulham call moussa dembéle’s £m mov...  \n",
              "4       starttoken northampton kieran brooke drafted e...  \n",
              "...                                                   ...  \n",
              "148719  starttoken marble head hercules pulled roman s...  \n",
              "148720  starttoken ‘i got sick talking myself’ spacey ...  \n",
              "148721  starttoken small town big potato inspired glob...  \n",
              "148722  starttoken power ban citizen entering australi...  \n",
              "148723  starttoken liberal mp say peter dutton let par...  \n",
              "\n",
              "[148695 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eea36b58-8e77-40d7-b45e-7c4ec6ea2a3b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>sectionName</th>\n",
              "      <th>webTitle</th>\n",
              "      <th>webUrl</th>\n",
              "      <th>bodyContent</th>\n",
              "      <th>webPublicationDate</th>\n",
              "      <th>id</th>\n",
              "      <th>webTitleToken</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>world/2016/jan/31/church-christian-british-adu...</td>\n",
              "      <td>World news</td>\n",
              "      <td>half british adult visited church past year</td>\n",
              "      <td>https://www.theguardian.com/world/2016/jan/31/...</td>\n",
              "      <td>half british adult visited church past year de...</td>\n",
              "      <td>2016-01-31T22:00:09Z</td>\n",
              "      <td>21.0</td>\n",
              "      <td>starttoken half british adult visited church p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tv-and-radio/2016/jan/31/war-and-peace-recap-e...</td>\n",
              "      <td>Television &amp; radio</td>\n",
              "      <td>war peace recap episode five – hero leech cast...</td>\n",
              "      <td>https://www.theguardian.com/tv-and-radio/2016/...</td>\n",
              "      <td>‘strange thing turn sometimes …’ pierre get aw...</td>\n",
              "      <td>2016-01-31T22:00:09Z</td>\n",
              "      <td>22.0</td>\n",
              "      <td>starttoken war peace recap episode five – hero...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport/2016/jan/31/angelique-kerber-serena-will...</td>\n",
              "      <td>Sport</td>\n",
              "      <td>angelique kerber aim dislodge serena williams ...</td>\n",
              "      <td>https://www.theguardian.com/sport/2016/jan/31/...</td>\n",
              "      <td>gone midnight angelique kerber conducting yet ...</td>\n",
              "      <td>2016-01-31T21:59:09Z</td>\n",
              "      <td>23.0</td>\n",
              "      <td>starttoken angelique kerber aim dislodge seren...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>football/2016/jan/31/fulham-call-off-moussa-de...</td>\n",
              "      <td>Football</td>\n",
              "      <td>fulham call moussa dembéle’s £m move tottenham...</td>\n",
              "      <td>https://www.theguardian.com/football/2016/jan/...</td>\n",
              "      <td>fulham angrily called moussa dembélé’s propose...</td>\n",
              "      <td>2016-01-31T21:56:54Z</td>\n",
              "      <td>24.0</td>\n",
              "      <td>starttoken fulham call moussa dembéle’s £m mov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sport/2016/jan/31/kieran-brookes-england-squad</td>\n",
              "      <td>Sport</td>\n",
              "      <td>northampton kieran brooke drafted england squa...</td>\n",
              "      <td>https://www.theguardian.com/sport/2016/jan/31/...</td>\n",
              "      <td>england drafted northampton prop kieran brooke...</td>\n",
              "      <td>2016-01-31T21:52:14Z</td>\n",
              "      <td>25.0</td>\n",
              "      <td>starttoken northampton kieran brooke drafted e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148719</th>\n",
              "      <td>world/2022/jun/21/marble-head-of-hercules-pull...</td>\n",
              "      <td>World news</td>\n",
              "      <td>marble head hercules pulled roman shipwreck si...</td>\n",
              "      <td>https://www.theguardian.com/world/2022/jun/21/...</td>\n",
              "      <td>archaeologist it’s underwater find keep giving...</td>\n",
              "      <td>2022-06-21T17:31:32Z</td>\n",
              "      <td>149835.0</td>\n",
              "      <td>starttoken marble head hercules pulled roman s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148720</th>\n",
              "      <td>music/2022/jun/22/i-got-sick-of-talking-about-...</td>\n",
              "      <td>Music</td>\n",
              "      <td>‘i got sick talking myself’ spacey jane back m...</td>\n",
              "      <td>https://www.theguardian.com/music/2022/jun/22/...</td>\n",
              "      <td>mop curl caleb harper – spacey jane frontman g...</td>\n",
              "      <td>2022-06-21T17:30:09Z</td>\n",
              "      <td>149836.0</td>\n",
              "      <td>starttoken ‘i got sick talking myself’ spacey ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148721</th>\n",
              "      <td>australia-news/2022/jun/22/the-small-town-with...</td>\n",
              "      <td>Australia news</td>\n",
              "      <td>small town big potato inspired global poetry win</td>\n",
              "      <td>https://www.theguardian.com/australia-news/202...</td>\n",
              "      <td>robertson small pretty town perched edge new s...</td>\n",
              "      <td>2022-06-21T17:30:09Z</td>\n",
              "      <td>149837.0</td>\n",
              "      <td>starttoken small town big potato inspired glob...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148722</th>\n",
              "      <td>australia-news/2022/jun/22/power-to-ban-citize...</td>\n",
              "      <td>Australia news</td>\n",
              "      <td>power ban citizen entering australia questione...</td>\n",
              "      <td>https://www.theguardian.com/australia-news/202...</td>\n",
              "      <td>high court decision striking home affair minis...</td>\n",
              "      <td>2022-06-21T17:30:08Z</td>\n",
              "      <td>149838.0</td>\n",
              "      <td>starttoken power ban citizen entering australi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148723</th>\n",
              "      <td>australia-news/2022/jun/22/liberal-mps-say-pet...</td>\n",
              "      <td>Australia news</td>\n",
              "      <td>liberal mp say peter dutton let party room dec...</td>\n",
              "      <td>https://www.theguardian.com/australia-news/202...</td>\n",
              "      <td>liberal mp urging peter dutton let party room ...</td>\n",
              "      <td>2022-06-21T17:30:08Z</td>\n",
              "      <td>149839.0</td>\n",
              "      <td>starttoken liberal mp say peter dutton let par...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>148695 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eea36b58-8e77-40d7-b45e-7c4ec6ea2a3b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eea36b58-8e77-40d7-b45e-7c4ec6ea2a3b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eea36b58-8e77-40d7-b45e-7c4ec6ea2a3b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-93ff180e-fa20-4871-92aa-547a0c7d1723\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-93ff180e-fa20-4871-92aa-547a0c7d1723')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-93ff180e-fa20-4871-92aa-547a0c7d1723 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function for advanced text cleaning\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstruct the text from tokens\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply advanced text preprocessing\n",
        "data['webTitle'] = data['webTitle'].apply(clean_text)\n",
        "data['bodyContent'] = data['bodyContent'].apply(clean_text)\n",
        "\n",
        "# Add <start> and <end> tokens to each headline\n",
        "data['webTitle'] = data['webTitle'].apply(lambda x: '<start> ' + x + ' <end>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430d22cd-dfa7-4556-e8ec-fed122f8a5cd",
        "id": "2hG_70vsi10R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2bf8ef1-9af7-419e-e61e-bc8b51f7479b",
        "id": "Hj9pNus8MEk3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function for advanced text cleaning\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstruct the text from tokens\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be6GGhPcSsuI"
      },
      "outputs": [],
      "source": [
        "def clean_text2(text):\n",
        "    # Remove specific unwanted characters\n",
        "    text = text.replace('”', '').replace('–', '')\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jykjN0dWSzP2"
      },
      "outputs": [],
      "source": [
        "data['webTitle'] = data['webTitle'].apply(clean_text2)\n",
        "data['bodyContent'] = data['bodyContent'].apply(clean_text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t3rK5K3vKhZ"
      },
      "outputs": [],
      "source": [
        "# Initialize the tokenizer without specifying `num_words`\n",
        "full_tokenizer = Tokenizer()\n",
        "full_tokenizer.fit_on_texts(data['webTitleToken'])\n",
        "full_tokenizer.fit_on_texts(data['bodyContent'])\n",
        "\n",
        "# Get word counts\n",
        "word_counts = full_tokenizer.word_counts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['webTitle']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577ae6b3-f0bc-4362-dabb-9752f4009aef",
        "id": "OfxZOk8Fi10R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118646    <start> demand tory mp scrap covid rule could ...\n",
              "124284    <start> really smart motorway would lower spee...\n",
              "141061    <start> ‘profiting suffering’ ap cancel sale m...\n",
              "80327     <start> school prison pipeline criminal justic...\n",
              "96867     <start> naked coronavirus tale desperation mad...\n",
              "                                ...                        \n",
              "140283    <start> uk new zealand sign free trade deal <end>\n",
              "110667    <start> crisis stage texas border city reel co...\n",
              "1812      <start> apple iphone sale projected stagnant q...\n",
              "91415      <start> woke gammon buzzword people coined <end>\n",
              "31590     <start> arsène wenger say arsenal fa cup win a...\n",
              "Name: webTitle, Length: 1486, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer with a smaller vocabulary\n",
        "max_vocab_size = 50  # Adjust as needed\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
        "tokenizer.fit_on_texts(data['webTitle'])\n",
        "tokenizer.fit_on_texts(data['bodyContent'])\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences_body = tokenizer.texts_to_sequences(data['bodyContent'])\n",
        "sequences_title = tokenizer.texts_to_sequences(data['webTitle'])\n",
        "\n",
        "# Padding sequences\n",
        "body_padded = pad_sequences(sequences_body, maxlen=300)  # Adjust maxlen as per your data\n",
        "title_padded = pad_sequences(sequences_title, maxlen=30, padding='post')  # Adjust maxlen as per your data\n",
        "\n",
        "# Preparing decoder input\n",
        "title_padded_shifted = np.zeros_like(title_padded)\n",
        "title_padded_shifted[:, 1:] = title_padded[:, :-1]"
      ],
      "metadata": {
        "id": "8Uyzmusni10R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into training and validation sets\n",
        "X_train, X_val, title_train, title_val, decoder_input_train, decoder_input_val = train_test_split(\n",
        "    body_padded, title_padded, title_padded_shifted, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "1Y5ZDc6Ui10R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert to sparse categorical\n",
        "def to_sparse_categorical(y, num_classes):\n",
        "    y_sparse = []\n",
        "    for sequence in y:\n",
        "        y_sparse_seq = np.zeros((len(sequence), num_classes))\n",
        "        for i, idx in enumerate(sequence):\n",
        "            if idx < num_classes:\n",
        "                y_sparse_seq[i, idx] = 1\n",
        "        y_sparse.append(y_sparse_seq)\n",
        "    return np.array(y_sparse)\n",
        "\n",
        "# Converting training and validation target data to sparse categorical\n",
        "num_classes = max_vocab_size + 1  # +1 for the padding token\n",
        "y_train_sparse = to_sparse_categorical(title_train, num_classes)\n",
        "y_val_sparse = to_sparse_categorical(title_val, num_classes)"
      ],
      "metadata": {
        "id": "H3eR1eb5MKoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Bidirectional\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(input_dim=max_vocab_size + 1, output_dim=128)\n",
        "encoder_lstm = Bidirectional(LSTM(128, return_state=True))\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding_layer = Embedding(input_dim=max_vocab_size + 1, output_dim=128)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_dense = Dense(max_vocab_size + 1, activation='softmax')  # Correct number of units\n",
        "\n",
        "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Seq2Seq Model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n"
      ],
      "metadata": {
        "id": "pa65WJMqBu9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "PV8zb5aEDYoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([X_train, decoder_input_train], y_train_sparse,\n",
        "                    validation_data=([X_val, decoder_input_val], y_val_sparse),\n",
        "                    epochs=5,  # Adjust as needed\n",
        "                    batch_size=32)"
      ],
      "metadata": {
        "id": "85csh9VyBu6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, load_model\n",
        "import pickle\n",
        "#saving model\n",
        "model.save('/content/drive/MyDrive/path/Models/seq2seq_headline_generator/seq2seq_headline_generator.h5')\n",
        "with open('/content/drive/My Drive/path/Models/seq2seq_headline_generator/history.p', 'wb') as f:\n",
        "    pickle.dump(history.history, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9EsR7nAizNj",
        "outputId": "c5f63c55-828b-4124-d308-801b5b62d028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wk_iFz07GIr",
        "outputId": "385b818c-6f9e-4afc-ac9a-a13c6430bc5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from scipy import sparse\n",
        "import string\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(2)\n",
        "from numpy.random import seed\n",
        "seed(1)"
      ],
      "metadata": {
        "id": "vKIdY35l7JJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"https://drive.google.com/drive/folders/1_RGwmjvW8p3jHuKqIbbHXdBDuDl2DTjU?usp=drive_link\""
      ],
      "metadata": {
        "id": "HinlvqOH7jv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder (data_url, quiet=True, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBGscuMy7g3F",
        "outputId": "c1bec2a4-c37c-4c44-8965-a9f90a797ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Guardian News Articles/guardian_articles.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data_sample = pd.read_csv('/content/Guardian News Articles/guardian_articles.csv')\n",
        "\n",
        "# Using only 10% of the data\n",
        "data = data_sample.sample(frac=0.01, random_state=42)\n",
        "\n",
        "# Drop rows with missing values\n",
        "data.dropna(subset=['webTitle', 'bodyContent'], inplace=True)"
      ],
      "metadata": {
        "id": "tVIY5vNwBvBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function for advanced text cleaning\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstruct the text from tokens\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply advanced text preprocessing\n",
        "data['webTitle'] = data['webTitle'].apply(clean_text)\n",
        "data['bodyContent'] = data['bodyContent'].apply(clean_text)\n",
        "\n",
        "# Add <start> and <end> tokens to each headline\n",
        "data['webTitle'] = data['webTitle'].apply(lambda x: '<start> ' + x + ' <end>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xm3KbhbG1R7",
        "outputId": "430d22cd-dfa7-4556-e8ec-fed122f8a5cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['webTitle']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8QZB61IBUj9",
        "outputId": "577ae6b3-f0bc-4362-dabb-9752f4009aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118646    <start> demand tory mp scrap covid rule could ...\n",
              "124284    <start> really smart motorway would lower spee...\n",
              "141061    <start> ‘profiting suffering’ ap cancel sale m...\n",
              "80327     <start> school prison pipeline criminal justic...\n",
              "96867     <start> naked coronavirus tale desperation mad...\n",
              "                                ...                        \n",
              "140283    <start> uk new zealand sign free trade deal <end>\n",
              "110667    <start> crisis stage texas border city reel co...\n",
              "1812      <start> apple iphone sale projected stagnant q...\n",
              "91415      <start> woke gammon buzzword people coined <end>\n",
              "31590     <start> arsène wenger say arsenal fa cup win a...\n",
              "Name: webTitle, Length: 1486, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer with a smaller vocabulary\n",
        "max_vocab_size = 50  # Adjust as needed\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
        "tokenizer.fit_on_texts(data['webTitle'])\n",
        "tokenizer.fit_on_texts(data['bodyContent'])\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences_body = tokenizer.texts_to_sequences(data['bodyContent'])\n",
        "sequences_title = tokenizer.texts_to_sequences(data['webTitle'])\n",
        "\n",
        "# Padding sequences\n",
        "body_padded = pad_sequences(sequences_body, maxlen=300)  # Adjust maxlen as per your data\n",
        "title_padded = pad_sequences(sequences_title, maxlen=30, padding='post')  # Adjust maxlen as per your data\n",
        "\n",
        "# Preparing decoder input\n",
        "title_padded_shifted = np.zeros_like(title_padded)\n",
        "title_padded_shifted[:, 1:] = title_padded[:, :-1]"
      ],
      "metadata": {
        "id": "i7WKnqyKFzib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into training and validation sets\n",
        "X_train, X_val, title_train, title_val, decoder_input_train, decoder_input_val = train_test_split(\n",
        "    body_padded, title_padded, title_padded_shifted, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7YyvwRBig9YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert to sparse categorical\n",
        "def to_sparse_categorical(y, num_classes):\n",
        "    y_sparse = []\n",
        "    for sequence in y:\n",
        "        y_sparse_seq = np.zeros((len(sequence), num_classes))\n",
        "        for i, idx in enumerate(sequence):\n",
        "            if idx < num_classes:\n",
        "                y_sparse_seq[i, idx] = 1\n",
        "        y_sparse.append(y_sparse_seq)\n",
        "    return np.array(y_sparse)\n",
        "\n",
        "# Converting training and validation target data to sparse categorical\n",
        "num_classes = max_vocab_size + 1  # +1 for the padding token\n",
        "y_train_sparse = to_sparse_categorical(title_train, num_classes)\n",
        "y_val_sparse = to_sparse_categorical(title_val, num_classes)"
      ],
      "metadata": {
        "id": "1rHkd8E3kIDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Bidirectional\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(input_dim=max_vocab_size + 1, output_dim=128)\n",
        "encoder_lstm = Bidirectional(LSTM(128, return_state=True))\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding_layer = Embedding(input_dim=max_vocab_size + 1, output_dim=128)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_dense = Dense(max_vocab_size + 1, activation='softmax')  # Correct number of units\n",
        "\n",
        "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Seq2Seq Model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n"
      ],
      "metadata": {
        "id": "9o2K9j0OkIDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "tatjjlrAkIDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([X_train, decoder_input_train], y_train_sparse,\n",
        "                    validation_data=([X_val, decoder_input_val], y_val_sparse),\n",
        "                    epochs=5,  # Adjust as needed\n",
        "                    batch_size=32)"
      ],
      "metadata": {
        "id": "0scruSA0kIDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, load_model\n",
        "import pickle\n",
        "#saving model\n",
        "model.save('/content/drive/MyDrive/path/Models/seq2seq_headline_generator/seq2seq_headline_generator.h5')\n",
        "with open('/content/drive/My Drive/path/Models/seq2seq_headline_generator/history.p', 'wb') as f:\n",
        "    pickle.dump(history.history, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f63c55-828b-4124-d308-801b5b62d028",
        "id": "dO0cHHOKkIDK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model, Model\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/path/Models/seq2seq_headline_generator/seq2seq_headline_generator.h5')\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('/content/drive/MyDrive/path/Models/seq2seq_headline_generator/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "metadata": {
        "id": "c1wvPfTgslCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder Model\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "\n",
        "# Decoder Model setup\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Set up the decoder, using `decoder_states_inputs` as initial state\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding_layer(decoder_inputs), initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
      ],
      "metadata": {
        "id": "zIh5JbuxBu0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Start with a sequence containing just the start token for the decoder\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = tokenizer.word_index['<start>']  # Assuming '<start>' is in the tokenizer\n",
        "\n",
        "\n",
        "    # Create the translation\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token and add the corresponding word to the decoded sentence\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = tokenizer.index_word[sampled_token_index]\n",
        "        decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        # Exit condition: either hit max length or find the stop token\n",
        "        if (sampled_word == '<end>' or len(decoded_sentence) > 50):  # Adjust max length as needed\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n"
      ],
      "metadata": {
        "id": "BmeZSg44BuxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = X_train[0:1]  # Take one sequence from the training set\n",
        "input_seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGA35MmSBut0",
        "outputId": "31994b27-6b81-4bfe-c99e-0dee2128aec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  592,\n",
              "           5, 3871,  869,    3, 4074, 1042,  162,  596,  406, 2993, 3871,\n",
              "          19,  748,  152,  817, 1769,    8, 2993,  415,    5,  745,  143,\n",
              "         587, 1193,  695,  407,  549,  293, 1535, 3019,  233,  698, 2177,\n",
              "          90, 1949, 4568,  417, 3979,  509, 3101,  481,  781,    1,  932,\n",
              "        3024,  592,  745,    3, 2169, 2404,  976,  721,  596, 1596,  345,\n",
              "        1817, 3822, 3817,  596,  463,  866,   91, 1388,  420, 3817,  592,\n",
              "         721,  431,  596,  608,  708,  497,  661, 4490, 3024,  592,   89,\n",
              "         792, 4184,  120,  535,    3,  592, 2455,  576,  949,  337, 1042,\n",
              "        1792, 3154,  254,  625, 2278,  769,  698,  346,  708,   25,  193,\n",
              "         698,    1,   66,  123,  150,  254, 4304,   51,  498,    5,   60,\n",
              "          45,   51, 2404,  393,   70,    7,  340,  395,  101,  305,  523,\n",
              "         500, 2278, 1020, 2589,  231,   45,   14,  777,  776, 1049, 4836,\n",
              "          52,   13,  771, 4915,  761,    1,  407, 1694,  207, 2082,  588,\n",
              "         879, 4979,    3,  592, 1520, 4036,  556,  932, 1070, 2404, 1194,\n",
              "         523, 1769,  524,  146,  868,  685, 2168, 2641,  320,  745,  463,\n",
              "         740,  789,    3]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if '<start>' in tokenizer.word_index:\n",
        "    print(\"'<start>' token is in the tokenizer's word index.\")\n",
        "else:\n",
        "    print(\"'<start>' token is NOT in the tokenizer's word index.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq8xfGv83_RJ",
        "outputId": "6d62dfe0-2465-43e3-fbcd-176596baa268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'<start>' token is NOT in the tokenizer's word index.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(tokenizer.word_index.items())[:50])  # Print first 50 entries of the tokenizer's word index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Jh5_xha4EVZ",
        "outputId": "19bc8fca-5dcf-4096-ae6d-b58d9e0634c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('”', 1), ('–', 2), ('said', 3), ('year', 4), ('one', 5), ('would', 6), ('people', 7), ('time', 8), ('end', 9), ('say', 10), ('new', 11), ('also', 12), ('start', 13), ('like', 14), ('u', 15), ('first', 16), ('it’s', 17), ('could', 18), ('two', 19), ('day', 20), ('last', 21), ('government', 22), ('make', 23), ('get', 24), ('way', 25), ('work', 26), ('back', 27), ('many', 28), ('even', 29), ('week', 30), ('world', 31), ('life', 32), ('uk', 33), ('may', 34), ('home', 35), ('right', 36), ('need', 37), ('made', 38), ('much', 39), ('take', 40), ('go', 41), ('“i', 42), ('still', 43), ('country', 44), ('thing', 45), ('well', 46), ('woman', 47), ('three', 48), ('think', 49), ('don’t', 50)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_headline = decode_sequence(input_seq)\n",
        "print(generated_headline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "nP3kfoDatijH",
        "outputId": "e163ec0e-a272-4371-bd1a-7672bddbd505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-ad146301e1b7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated_headline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_headline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-f99c6bf2b219>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtarget_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<start>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'<start>' token not found in tokenizer's word index.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Create the translation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"'<start>' token not found in tokenizer's word index.\""
          ]
        }
      ]
    }
  ]
}