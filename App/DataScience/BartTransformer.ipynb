{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wk_iFz07GIr",
        "outputId": "385b818c-6f9e-4afc-ac9a-a13c6430bc5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from scipy import sparse\n",
        "import string\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFBertModel, TFBertForSequenceClassification\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout,TimeDistributed,Bidirectional\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(2)\n",
        "from numpy.random import seed\n",
        "seed(1)"
      ],
      "metadata": {
        "id": "vKIdY35l7JJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtQbvBxDIq4y"
      },
      "source": [
        "This code block is used to mount your Google Drive to the Colab environment. It allows you to access files stored in your Google Drive directly from the Colab notebook. The force_remount=True parameter ensures that the drive is remounted if it was previously mounted during the session. This is useful for ensuring that the latest version of the drive's contents is accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c72c502-3137-402a-e493-6fafd59f4cc5",
        "id": "8BvGpnYxcAcX"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TsVvmSjIzYL"
      },
      "source": [
        "This code block imports essential libraries and modules for data processing, machine learning model building, and text analysis. It includes libraries for data manipulation (Pandas), neural network construction (Keras, TensorFlow), natural language processing (NLTK), and various utilities for handling arrays, strings, and file paths. Additionally, it sets the seed for random number generation to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQkYquo-cAcY"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from scipy import sparse\n",
        "import string\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(2)\n",
        "from numpy.random import seed\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"https://drive.google.com/drive/folders/1_RGwmjvW8p3jHuKqIbbHXdBDuDl2DTjU?usp=drive_link\""
      ],
      "metadata": {
        "id": "HinlvqOH7jv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder (data_url, quiet=True, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBGscuMy7g3F",
        "outputId": "c1bec2a4-c37c-4c44-8965-a9f90a797ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Guardian News Articles/guardian_articles.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmEYcDKJl6Vj"
      },
      "outputs": [],
      "source": [
        "pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/Guardian News Articles/guardian_articles.csv')\n",
        "\n",
        "# Basic preprocessing\n",
        "data['bodyContent'] = data['bodyContent'].apply(lambda x: '[CLS] ' + x + ' [SEP]')\n",
        "data['webTitle'] = data['webTitle'].apply(lambda x: '[CLS] ' + x + ' [SEP]')\n",
        "\n",
        "# Splitting the dataset\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(data['bodyContent'], data['webTitle'], test_size=0.2)\n"
      ],
      "metadata": {
        "id": "vrlju6SKl_X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=512)\n"
      ],
      "metadata": {
        "id": "jSlNNCt8mch6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))\n"
      ],
      "metadata": {
        "id": "OPsuXQ1dmdDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT\n",
        "bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Build a custom model\n",
        "input_ids = tf.keras.Input(shape=(512,), dtype='int32', name='input_ids')  # Adjust the shape as needed\n",
        "attention_mask = tf.keras.Input(shape=(512,), dtype='int32', name='attention_mask')  # Adjust the shape as needed\n",
        "\n",
        "# BERT output\n",
        "bert_output = bert(input_ids, attention_mask=attention_mask)[0]  # Using the last hidden state\n",
        "\n",
        "# BiLSTM Decoder\n",
        "decoder_bilstm = Bidirectional(LSTM(256, return_sequences=True, return_state=True))\n",
        "decoder_outputs, forward_h, forward_c, backward_h, backward_c = decoder_bilstm(bert_output)\n",
        "\n",
        "# Dense layer for output\n",
        "decoder_dense = Dense(num_classes, activation='softmax')  # num_classes is the size of your target vocabulary\n",
        "decoder_outputs = TimeDistributed(decoder_dense)(decoder_outputs)\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "xLDkfRafmhoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)"
      ],
      "metadata": {
        "id": "EdOquUPvmpnv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}